<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ThreadLocal用法及原理]]></title>
    <url>%2FThreadLocal%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[与Synchonized的对照: ThreadLocal和Synchonized都用于解决多线程并发訪问。可是ThreadLocal与synchronized有本质的差别。synchronized是利用锁的机制，使变量或代码块在某一时该仅仅能被一个线程訪问。而ThreadLocal为每个线程都提供了变量的副本，使得每个线程在某一时间訪问到的并非同一个对象，这样就隔离了多个线程对数据的数据共享。而Synchronized却正好相反，它用于在多个线程间通信时可以获得数据共享。 synchronized用于线程间的数据贡献，而ThreadLocal则用于线程间的数据隔离。 用法把要线程隔离的数据放进ThreadLocal 。 12345static ThreadLocal&lt;T&gt; threadLocal = new ThreadLocal&lt;T&gt;() &#123; protected T initialValue() &#123; 这里一般new一个对象返回 &#125;&#125; 线程获取相关数据的时候只要 1threadLocal.get(); 想修改、赋值只要 1threadLocal.set(val) 使用场景如上面说到的，ThreadLocal是用于线程间的数据隔离，ThreadLocal为每个线程都提供了变量的副本。 举例1：联想一下服务器（例如tomcat）处理请求的时候，会从线程池中取一条出来进行处理请求，如果想把每个请求的用户信息保存到一个静态变量里以便在处理请求过程中随时获取到用户信息。这时候可以建一个拦截器，请求到来时，把用户信息存到一个静态ThreadLocal变量中，那么在请求处理过程中可以随时从静态ThreadLocal变量获取用户信息。 举例2：Spring的事务实现也借助了ThreadLocal类。Spring会从数据库连接池中获得一个connection，然会把connection放进ThreadLocal中，也就和线程绑定了，事务需要提交或者回滚，只要从ThreadLocal中拿到connection进行操作。 原理分析1、get()用法 1234567891011121314151617public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123;//当map已存在 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();//初始化值&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 上面先取到当前线程，然后调用getMap方法获取对应的ThreadLocalMap，ThreadLocalMap是ThreadLocal的静态内部类，然后Thread类中有一个这样类型成员，所以getMap是直接返回Thread的成员 1ThreadLocal.ThreadLocalMap threadLocals = null; 来看下ThreadLocal的内部类ThreadLocalMap源码，留个大致印象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static class ThreadLocalMap &#123; private static final int INITIAL_CAPACITY = 16;//初始数组大小 private Entry[] table;//每个可以拥有多个ThreadLocal private int size = 0; private int threshold;//扩容阀值 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); &#125; private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; //循环利用key过期的Entry replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125;｝ 可以看到有个Entry内部静态类，它继承了WeakReference，总之它记录了两个信息，一个是ThreadLocal&lt;?&gt;类型，一个是Object类型的值。getEntry方法则是获取某个ThreadLocal对应的值，set方法就是更新或赋值相应的ThreadLocal对应的值。里面涉及到扩容策略、Entry哈希冲突、循环利用等等不再深入，留个大致印象就好。 回顾下get()方法中的代码 123456789if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result;7 &#125;&#125;return setInitialValue(); map为null或e为null就会走到setInitialValue，如果我们是第一次get()方法，那map会是空的，所以接下来先看setInitialValue()方法 123456789101112private T setInitialValue() &#123; //调用我们实现的方法得到需要线程隔离的值 T value = initialValue(); Thread t = Thread.currentThread(); //拿到相应线程的ThreadLocalMap成员变量 ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 上面initialValue就是实例化ThreadLocal要实现的方法，这里又取了线程的ThreadLocalMap，不为空就把值set进去（键为TreadLocal本身，值就是initialValue返回的值）；为空就创建一个map同时添加一个值进去,最后返回value。 map.set(this, value)这句代码在上面的ThreadLocalMap源码中可以看到大致流程，下面看看createMap()做了什么事 12345678910111213void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); //创建一个Entry，加入数组 table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY);&#125; 可以看到在new ThreadLocalMap之后，就会创建一个Entry加入到数组中，最后把ThreadLocalMap的引用赋值给Thread的threadLocals成员变量 在回顾下get()方法中的代码 123456789if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125;&#125;return setInitialValue(); 现在map不会为空了，再次调用get方法就会调用map的getEntry方法（上面的ThreadLocalMap源码中可以看到大致流程），拿到相应的Entry，然后就可以拿到相应的值返回出去 2、set()方法 分析完get()方法，那么set()方法就自然而然的明白了，就不再赘述 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 总结原理ThreadLocal的实现原理是，在每个线程中维护一个Map，键是ThreadLocal类型，值是Object类型。当想获取ThreadLocal的值时，就从当前线程中拿出Map，然后在把ThreadLocal本身作为键从Map中拿出值返回。 优缺点优点 提供线程内的局部变量。每个线程都自己管理自己的局部变量，互不影响 缺点 内存泄漏问题。可以看到ThreadLocalMap中的Entry是继承WeakReference的，其中ThreadLocal是以弱引用形式存在Entry中，如果ThreadLocal在外部没有被强引用，那么垃圾回收的时候就会被回收掉，又因为Entry中的value是强引用，就会出现内存泄漏。虽然ThreadLocal源码中的会对这种情况进行了处理，但还是建议不需要用TreadLocal的时候，手动调remove方法。]]></content>
      <categories>
        <category>Java</category>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java高并发</tag>
        <tag>多线程</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈HTTP中Get与Post的区别]]></title>
    <url>%2F%E6%B5%85%E8%B0%88HTTP%E4%B8%ADGet%E4%B8%8EPost%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[HTTP定义了与服务器交互的不同方法，最基本的方法通常是GET、POST、PUT和DELETE四种。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源。而HTTP中的四种基本操作，就对应于这个资源的增删改查。因此可以先简单理解为GET请求用于查询/获取资源信息，而POST请求一般用于新增资源信息。另外的PUT请求用于更新资源信息，DELETE请求用于删除个人信息。 1、根据HTTP规范，GET用于信息获取，而且应该是安全和幂等的（1）所谓的安全意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不会产生有害的副作用，就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改、增加数据，不会影响资源状态。 注意：这里的安全的含义仅仅是指不修改信息。 （2）幂等意味着对同一URL的多个请求应该返回同样的结果。 幂等（idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有一下几种定义： ​ 对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a)=abs(abs(a))。 ​ 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x。 看完上述幂等的含义后，应该可以理解GET请求中的幂等含义的。 但是在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 2、根据HTTP规范，POST表示可能修改服务器上的资源的请求继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下HTTP规范中GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说： 1.很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。 2.对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。 3.另外一个是，早期的Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。 以上3点典型地描述了老一套的风格（没有严格遵守HTTP规范），随着架构的发展，现在出现REST(Representational State Transfer)，一套支持HTTP规范的新风格，这里不多说了，可以参考《RESTful Web Services》。 说完原理性的问题，我们再从表面现象上面看看GET和POST的区别： 1.GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0%E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如：%E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST把提交的数据则放置在是HTTP包的Request Body（请求体）中。 2.”GET方式提交的数据最多只能是1024字节，理论上POST没有限制，可传较大量的数据，IIS4中最大为80KB，IIS5中为100KB”？？！ 以上这句是我从其他文章转过来的，其实这样说是错误的，不准确的： (1) 首先是”GET方式提交的数据最多只能是1024字节”，因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系了。而实际上，URL不存在参数上限的问题，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制。IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 注意这是限制是整个URL长度，而不仅仅是你的参数值数据长度。 (2) 理论上讲，POST是没有大小限制的，HTTP协议规范也没有进行大小限制，说“POST数据量存在80K/100K的大小限制”是不准确的，POST数据是没有限制的，起限制作用的是服务器的处理程序的处理能力。 对于ASP程序，Request对象处理每个表单域时存在100K的数据长度限制。但如果使用Request.BinaryRead则没有这个限制。 由这个延伸出去，对于IIS 6.0，微软出于安全考虑，加大了限制。我们还需要注意： 1).IIS 6.0默认ASP POST数据量最大为200KB，每个表单域限制是100KB。 2).IIS 6.0默认上传文件的最大大小是4MB。 3).IIS 6.0默认最大请求头是16KB。 IIS 6.0之前没有这些限制。[见参考资料5] 所以上面的80K，100K可能只是默认值而已(注：关于IIS4和IIS5的参数，我还没有确认)，但肯定是可以自己设置的。由于每个版本的IIS对这些参数的默认值都不一样，具体请参考相关的IIS配置文档。 3.在ASP中，服务端获取GET请求参数用Request.QueryString，获取POST请求参数用Request.Form。在JSP中，用request.getParameter(\”XXXX\”)来获取，虽然jsp中也有request.getQueryString()方法，但使用起来比较麻烦，比如：传一个test.jsp?name=hyddd&amp;password=hyddd，用request.getQueryString()得到的是：name=hyddd&amp;password=hyddd。在PHP中，可以用$_GET和$_POST分别获取GET和POST中的数据，而$_REQUEST则可以获取GET和POST两种请求中的数据。值得注意的是，JSP中使用request和PHP中使用$_REQUEST都会有隐患。 4.POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存，(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击。 总结一下，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求，在FORM（表单）中，Method默认为”GET”，实质上，GET和POST只是发送机制不同，并不是一个取一个发！]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码分析]]></title>
    <url>%2FHashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashMap最早出现在JDK1.2中，底层基于散列算法（Hash）实现。并且HashMap允许null键和null值，是线程非安全类，在多线程环境下可能会存在问题，与之对应的是HashTable类，HashTable是线程安全的HashMap，但是在之前由于HashTable方法都是用synchronized实现，开销比较大，所以在多线程时反而并不使用HashTable，而是使用CurrentHashMap。在JDK1.8后，HashMap和CurrentHashMap的数据结构都有了新变化，即加入了红黑树。 JDK1.7及之前版本中的HashMap数据结构在JDK1.6，1.7中，HashMap的实现都是用基础的“拉链法”去实现，即数组+链表的形式，通过不同的hash值来对数据进行分配。关于更多1.7版本及之前的HashMap数据结构可以参考我的另一篇博客Java中的容器。 JDK1.8版本的HashMap数据结构 在JDK1.8中对HashMap的源码进行了优化，在JDK1.7中，HashMap处理“碰撞”的时候，都是采用链表来存储，当碰撞的节点很多的时候，查询时间复杂度是O(N)。而JDK1.8中，HashMap处理“碰撞”增加了红黑树这种数据结构，当碰撞节点较少时，采样链表存储，当较大（默认是&gt;8）时，变成使用红黑树（特点是查询时间是O(logN)）存储。 结构Node是HashMap中的一个静态内部类： 123456789101112131415161718192021222324252627282930313233343536373839404142//Node是单向链表，实现了Map.Entry接口static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; //构造函数 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; // getter and setter ... toString ... public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; TreeNode是红黑树的数据结构： 1234567891011121314151617181920static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; 类定义1public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 变量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 默认初始容量16(必须是2的幂次方) */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** * 最大容量，2的30次方 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认加载因子，用来计算threshold */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 链表转成树的阈值，当桶中链表长度大于8时转成树 threshold = capacity * loadFactor */static final int TREEIFY_THRESHOLD = 8;/** * 进行resize操作时，若桶中数量少于6则从树转成链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * 桶中结构转化为红黑树对应的table的最小大小 当需要将解决 hash 冲突的链表转变为红黑树时， 需要判断下此时数组容量， 若是由于数组容量太小（小于 MIN_TREEIFY_CAPACITY ） 导致的 hash 冲突太多，则不进行链表转变为红黑树操作， 转为利用 resize() 函数对 hashMap 扩容 */static final int MIN_TREEIFY_CAPACITY = 64;/** 保存Node&lt;K,V&gt;节点的数组 该表在首次使用时初始化，并根据需要调整大小。 分配时， 长度始终是2的幂。 */transient Node&lt;K,V&gt;[] table;/** * 存放具体元素的集 */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * 记录 hashMap 当前存储的元素的数量 */transient int size;/** * 每次更改map结构的计数器 */transient int modCount;/** * 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 */int threshold;/** * 负载因子：要调整大小的下一个大小值（容量*加载因子）。 */final float loadFactor; 构造方法12 tableSizeFor方法详解用位运算找到大于或等于cap的最小2的整次幂的数，比如10，则返回16。 让cap-1再赋值给n的目的是使得找到的目标值大于或等于原值。例如二进制0100,十进制是4,若不减1而直接操作，答案是0001 0000十进制是16，明显不符合预期。 对n右移1位：001xx…xxx，再位或：011xx…xxx 对n右移2位：00011…xxx，再位或：01111…xxx 对n右移4位… 对n右移8位… 对n右移16位,因为int最大就2^32所以移动1、2、4、8、16位并取位或,会将最高位的1后面的位全变为1。 再让结果n+1，即得到了2的整数次幂的值了。 附带一个实例： loadFactory负载因子对于HashMap来说，负载因子是一个很重要的参数，该参数反应了HashMap桶数组的使用情况，通过调节负载因子，可以使HashMap时间和空间复杂度上有不同的表现。 当我们调低负载因子时，HashMap 所能容纳的键值对数量变少。扩容时，重新将键值对存储新的桶数组里，键的键之间产生的碰撞会下降，链表长度变短。此时，HashMap 的增删改查等操作的效率将会变高，这里是典型的拿空间换时间。 相反，如果增加负载因子（负载因子可以大于1），HashMap 所能容纳的键值对数量变多，空间利用率高，但碰撞率也高。这意味着链表长度变长，效率也随之降低，这种情况是拿时间换空间。至于负载因子怎么调节，这个看使用场景了。 一般情况下，我们用默认值就可以了。大多数情况下0.75在时间跟空间代价上达到了平衡所以不建议修改。 查找123456789101112131415161718192021222324252627282930313233343536public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;// 获取hash值static final int hash(Object key) &#123; int h; // 拿到key的hash值后与其五符号右移16位取与 // 通过这种方式，让高位数据与低位数据进行异或，以此加大低位信息的随机性，变相的让高位数据参与到计算中。 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 定位键值对所在桶的位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断桶中第一项(数组元素)相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个结点 if ((e = first.next) != null) &#123; // 是否是红黑树，是的话调用getTreeNode方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 不是红黑树的话，在链表中遍历查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 注意： HashMap的hash算法(hash()方法)。 (n - 1) &amp;amp; hash等价于对 length 取余。 添加 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public V put(K key, V value) &#123; // 调用hash(key)方法来计算hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 容量初始化：当table为空，则调用resize()方法来初始化容器 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //确定元素存放在哪个桶中，桶为空，新生成结点放入桶中 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) //如果键的值以及节点 hash 等于链表中的第一个键值对节点时，则将 e 指向该键值对 e = p; // 如果桶中的引用类型为 TreeNode，则调用红黑树的插入方法 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //对链表进行遍历，并统计链表长度 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; //在尾部插入新结点 p.next = newNode(hash, key, value, null); // 如果结点数量达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //判断要插入的键值对是否存在 HashMap 中 if (e != null) &#123; // existing mapping for key V oldValue = e.value; // onlyIfAbsent 表示是否仅在 oldValue 为 null 的情况下更新键值对的值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 键值对数量超过阈值时，则进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 事实上，new HashMap();完成后，如果没有put操作，是不会分配存储空间的。 当桶数组 table 为空时，通过扩容的方式初始化 table 查找要插入的键值对是否已经存在，存在的话根据条件判断是否用新值替换旧值 如果不存在，则将键值对链入链表中，并根据链表长度决定是否将链表转为红黑树 判断键值对数量是否大于阈值，大于的话则进行扩容操作 扩容机制在 HashMap 中，桶数组的长度均是2的幂，阈值大小为桶数组长度与负载因子的乘积。当 HashMap 中的键值对数量超过阈值时，进行扩容。 HashMap 按当前桶数组长度的2倍进行扩容，阈值也变为原来的2倍（如果计算过程中，阈值溢出归零，则按阈值公式重新计算）。扩容之后，要重新计算键值对的位置，并把它们移动到合适的位置上去。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293final Node&lt;K,V&gt;[] resize() &#123; // 拿到数组桶 Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 如果数组桶的容量大与0 if (oldCap &gt; 0) &#123; // 如果比最大值还大，则赋值为最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 如果扩容后小于最大值 而且 旧数组桶大于初始容量16， 阈值左移1(扩大2倍) else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 如果数组桶容量&lt;=0 且 旧阈值 &gt;0 else if (oldThr &gt; 0) // initial capacity was placed in threshold // 新容量=旧阈值 newCap = oldThr; // 如果数组桶容量&lt;=0 且 旧阈值 &lt;=0 else &#123; // zero initial threshold signifies using defaults // 新容量=默认容量 newCap = DEFAULT_INITIAL_CAPACITY; // 新阈值= 负载因子*默认容量 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果新阈值为0 if (newThr == 0) &#123; // 重新计算阈值 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 更新阈值 threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) // 创建新数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 覆盖数组桶 table = newTab; // 如果旧数组桶不是空，则遍历桶数组，并将键值对映射到新的桶数组中 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果是红黑树 else if (e instanceof TreeNode) // 重新映射时，需要对红黑树进行拆分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 如果不是红黑树，则按链表处理 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; // 遍历链表，并将链表节点按原顺序进行分组 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将分组后的链表映射到新桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 整体步骤： 计算新桶数组的容量 newCap 和新阈值 newThr 根据计算出的 newCap 创建新的桶数组，桶数组 table 也是在这里进行初始化的 将键值对节点重新映射到新的桶数组里。如果节点是 TreeNode 类型，则需要拆分红黑树。如果是普通节点，则节点按原顺序进行分组。 总结起来，一共有三种扩容方式： 使用默认构造方法初始化HashMap。从前文可以知道HashMap在一开始初始化的时候会返回一个空的table，并且thershold为0。因此第一次扩容的容量为默认值DEFAULT_INITIAL_CAPACITY也就是16。同时threshold = DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR = 12。 指定初始容量的构造方法初始化HashMap。那么从下面源码可以看到初始容量会等于threshold，接着threshold = 当前的容量（threshold） * DEFAULT_LOAD_FACTOR。 HashMap不是第一次扩容。如果HashMap已经扩容过的话，那么每次table的容量以及threshold量为原有的两倍。 细心点的人会很好奇，为什么要判断loadFactor为0呢？ loadFactor小数位为 0，整数位可被2整除且大于等于8时，在某次计算中就可能会导致 newThr 溢出归零。 疑问和进阶1. JDK1.7是基于数组+单链表实现（为什么不用双链表） 首先，用链表是为了解决hash冲突。 单链表能实现为什么要用双链表呢?(双链表需要更大的存储空间) 2. 为什么要用红黑树，而不用平衡二叉树？ 插入效率比平衡二叉树高，查询效率比普通二叉树高。所以选择性能相对折中的红黑树。 3. 重写对象的Equals方法时，要重写hashCode方法，为什么？跟HashMap有什么关系？ equals与hashcode间的关系: 如果两个对象相同（即用equals比较返回true），那么它们的hashCode值一定要相同； 如果两个对象的hashCode相同，它们并不一定相同(即用equals比较返回false) 因为在 HashMap 的链表结构中遍历判断的时候，特定情况下重写的 equals 方法比较对象是否相等的业务逻辑比较复杂，循环下来更是影响查找效率。所以这里把 hashcode 的判断放在前面，只要 hashcode 不相等就玩儿完，不用再去调用复杂的 equals 了。很多程度地提升 HashMap 的使用效率。 所以重写 hashcode 方法是为了让我们能够正常使用 HashMap 等集合类，因为 HashMap 判断对象是否相等既要比较 hashcode 又要使用 equals 比较。而这样的实现是为了提高 HashMap 的效率。 4. HashMap为什么不直接使用对象的原始hash值呢? 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 我们发现，HashMap的哈希值是通过上面的方式获取，而不是通过key.hashCode()方法获取。 原因： 通过移位和异或运算，可以让 hash 变得更复杂，进而影响 hash 的分布性。 5. 既然红黑树那么好，为啥hashmap不直接采用红黑树，而是当大于8个的时候才转换红黑树？ 因为红黑树需要进行左旋，右旋操作， 而单链表不需要。 以下都是单链表与红黑树结构对比。 如果元素小于8个，查询成本高，新增成本低。 如果元素大于8个，查询成本低，新增成本高。 至于为什么选数字8，是大佬折中衡量的结果-.-，就像loadFactor默认值0.75一样。]]></content>
      <categories>
        <category>Java</category>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
        <tag>HashMap</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由一道面试题理解类加载机制]]></title>
    <url>%2F%E7%94%B1%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98%E7%90%86%E8%A7%A3%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[不了解JVM的类加载机制你也可以coding，但是当你了解之后，可以让你在coding的时候避免很多坑。本文将以一道常见的面试题去剖析一下JVM中的类加载机制。 本文参考周志明《深入理解JVM虚拟机》（第2版） 12345678910111213141516public class ClassLoadTest&#123; private static ClassLoaderTest test = new ClassLoad(); static int x; static int y = 0; public ClassLoaderTest()&#123; x++: y++; &#125; public static void main(String[] args)&#123; System.out.println(test.x); System.out.println(test.y); &#125;&#125; 那么上面代码运行的结果x，y值是多少呢？可以先猜测一下答案，结果可能会出乎你的意料~ 类加载过程： 先用一个图简单的描述一下类加载的这个过程 加载 这个过程相当于从本地或者网络端去读取一个字节流，然后将一些静态存储结构转化成方法区中运行时期的数据，最后生成一个代表整个类的Class对象，作为方法区访问这个类的入口。 例如： 咱们可以通过一个类的全限定名去加载类 通过jar、war包去加载类 通过http请求去第三方平台上拉取指定的类来加载 运行时计算生成，例如cglib动态代理等等 针对上述例子，这里是加载一个ClassLoaderTest.class对象 验证 要理解这个环节其实并不难，一个东西要放到JVM上去运行，咱们肯定得对其进行一些过滤，不能什么都往上丢，这也是JVM自我保护的一种机制，这里的验证简单的举几个例子。 文件格式的验证 ​ （1）是否以魔数0xCAFEBABE开头。 ​ （2）主次版本号是否在当前虚拟机处理范围内。 ​ （3）常量池中的常量是否有不被支持的常量类型等等。 元数据的验证 ​ （1）这个类是否有父类 ​ （2）这个类的父类是否继承了不被允许继承的类（final修饰的类） ​ （3）这个类不是抽象类，是否实现了所有接口中要实现的方法等等。 字节码的验证 ​ （1）保证跳转指令不会跳转到方法体以外的字节码指令上。 ​ （2）保证方法体中的类型转换是有效的等等。 符号引用的验证 ​ （1）能否通过类的全限定名去找到对应的类 ​ （2）符号引用中的类、字段、方法是否可以被当前类访问等等。 准备过程 这个过程相当于给类变量分配内存空间并设置变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。 针对上述例子： 123test = null;x = 0;y = 0; 解析过程 将符号引用转化成直接引用的过程。这个有两个名词 符号引用 和直接引用。 符号引用：符号引用与虚拟机的布局无关，甚至引用的目标不一定加载到了内存中。符号可以是任何形式的字面量，只要使用时能够准确的定位到目标即可。 直接引用：直接引用可以直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用与虚拟机布局有关，如果有了直接引用，那么引用的目标必定已经在内存中存在。 而解析过程又会针对类、字段、方法进行解析，解析失败则会抛出相应的异常。例如在解析时发现没有访问权限会抛出 java.lang.IllegalAccessException 异常，查询不到引用字段会抛出 java.lang.NoSuchFieldException 异常，查询不到方法会抛出 java.lang.NoSuchMethodException 异常等等。 初始化 在准备阶段，变量已经赋值过系统要求的默认值，在初始化阶段，则会根据程序制定的主观计划去初始化类变量和其他资源。这句话听起来有些绕口，根据上述例子，实际上就是： 12test = new ClassLoaderTest();y = 0; 这个过程，由于 x 咱们自己并没有去设定一个值，所以初始化阶段它不会发生任何改变,但是 y 咱们有设定一个值0，所以最后造成最终结果为: 12x = 1;y = 0; ps：在同一个类加载器下，一个类只会初始化一次。多个线程同时初始化一个类，只有一个线程能正常初始化，其他线程都会进行阻塞等待，直到活动线程执行初始化方法完毕。 总结 对于上面这个面试题，咱们用流程图简单的描述一下：]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>类加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个设计模式：工厂模式]]></title>
    <url>%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AA%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[创建型设计模式中的工厂模式可以细分为三种：分别是简单工厂模式、工厂方法模式、抽象工厂模式。 我们可以通过一个小例子更加形象的理解工厂上述三种模式。 无工厂时期简单工厂时期工厂方法时期抽象工厂方法时期（1）还没有工厂时代：假若还没有工业革命，如果客户想要一辆宝马车，一般的做法是他自己去创建一辆宝马车，然后拿来用。 （2）简单工厂模式：后来出现工业革命，客户不再自己创建宝马车，而是通过一个宝马工厂来帮他创建宝马车，比如想要320i系列车，工厂就可以创建这个系列的车，即工厂可以创建产品。 （3）工厂方法模式：为了满足客户需求，宝马车系列越来越多，如320i、523i、30li等系列一个工厂无法创建所有的宝马系列，于是单独分出来多个具体的工厂。每个具体的工厂创建一种系列。即具体工厂类只能创建一个具体的产品。但是宝马工厂还是抽象，你需要指定某个具体的工厂才能生产出车来。 （4）抽象工厂模式：随着客户的要求越来越高，宝马车必须配置空调，于是这个工厂开始生产宝马车和需要的空调。最终是客户只要对宝马的销售员说：我要523i空调车，销售员就直接给他523i空调车了。而不用自己去创建523i空调车宝马车。 这就是工厂模式。 下面一一讲解： 1、简单工厂模式定义：在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。 简单工厂模式属于类的创建型模式，又叫做静态工厂模式。通过专门定义一个工厂类来负责创建其它类的实例，被创建的实例通常都具有共同的父类。需要注意的是，简单工厂模式不属于23种GOF设计模式之一。 简单工厂模式是由一个工厂对象决定创建出哪一种产品类的实例，简单工厂模式是工厂模式家族种最简单实用的模式，可以理解为是不同工厂模式的一个特殊实现。 简单工厂模式包含三种角色： （1）工厂角色（Creator） 这是简单工厂模式的核心，它用来负责创建所有实例的内部逻辑。工厂类可以被外界直接调用，创建所需的产品对象。 （2）抽象角色（Product） 这是简单工厂模式所创建的所有对象的父类，它负责描述所有实例所共有的公共接口。该类可以是接口，也可以是抽象类。 （3）具体产品角色（Concrete Product） 简单工厂模式所创建的具体的实例对象。 以上面的UML为例，表示的是一个用简单工厂方法模式实现的加减乘除计算器程序。 其中Operator是一个抽象类，其中包含属性numberA及numberB，以及一个getResult( )方法用于返回计算结果。它的角色就是抽象角色（Product）。 下面的AddOperator、SubOperator、MulOperator、DivOperator是Operator的子类，分别代表加减乘除四种运算，他们的角色是具体产品角色（Concrete Peoduct）。 OperatorFactory是工厂类，其中的createOperator( )方法用于创建计算器对象。 12345678910111213/*** 计算器抽象类*/public abstract class Operator&#123; private double numberA; private double numberB; //获取结果的抽象方法 protected abstract double getResult() throws Exception; // getter和setter方法省略 ...&#125; 12345678910/** 加法计算类*/public class AddOperator extends Operator&#123; //实现父类的抽象方法 @Override protected double getResult()&#123; return getNumberA() + getNumberB(); &#125;&#125; 123456789/** 减法计算类*/public class SubOperator extends Operator&#123; @Override protected double getResult()&#123; return getNumberA() + getNumberB(); &#125;&#125; 123456789/** 乘法计算类*/public class MulOperator extends Operator&#123; @Override protected double getResult()&#123; return getNumberA() * getNumberB(); &#125;&#125; 12345678910111213/** 除法计算类*/public class MulOperator extends Operator&#123; @Override protected double getResult() throws Exception&#123; if(getNmberB() == 0.0)&#123; throw new Exception("除数不能为0"); &#125;else&#123; return getNumberA() / getNumberB(); &#125; &#125;&#125; 1234567891011121314151617181920212223/** * 简单工厂类 */public class OperatorFactory &#123; public static Operator createOperator(String operation)&#123; Operator operator = null; switch (operation)&#123; case "+": operator = new AddOperator(); break; case "-": operator = new SubOperator(); break; case "*": operator = new MulOperator(); break; case "/": operator = new DivOperator(); break; &#125; return operator; &#125;&#125; 123456789101112131415/** * 测试简单工厂类 */public class OperatorTest &#123; public static void main(String[] args) &#123; Operator operator = OperatorFactory.createOperator("+"); operator.setNumberA(10); operator.setNumberB(5); try &#123; System.out.println(operator.getResult()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 延伸：试想一下，当我们在coding的时候，如果在A类里面new了一个B类的对象，那么A类在某种程度上说就是依赖于B类。如果在后期开发的时候需求变化或者是维护的时候，需要修改B类的时候，我们就需要打开源代码修改所有与这个类有关的类了，做过重构的朋友就知道，这样的事情虽然无法避免，但是确实是一件让人心碎的事情。 通过以上分析，我们可以得出简单工厂模式的优缺点： 在简单工厂模式中，工厂类是整个模式的关键，它包含了必要的逻辑判断，能够根据外界给定的条件去判断应该创建哪个具体类的实例，用户使用时可以直接根据工厂类去创建所需的实例，而无需关心这些对象是如何组织并创建的，从这一点来说，这有利于整个软件体系结构的优化。 但是，简单工厂模式的缺点也正体现在工厂类上，由于工厂类集中了所有实例的创建逻辑，当我们增加一个新的具体类时，需要同时修改工厂类（多加几个if判断），这违反了”开闭原则“。 2、工厂方法模式定义：定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。 工厂方法模式又称多态性工厂模式，是对普通工厂方法模式的改进，在工厂方法模式中，核心的工厂类不再负责所有产品的创建，而是将具体创建的工作交给子类去做。该核心类成为一个抽象工厂角色，仅负责给出具体工厂子类必须实现的接口，而不接触哪一个产品类应当被实例化这种细节。 在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。 依旧以上面计算器为例： 我们对OperatorFactory类更改如下： 123456789101112131415161718public class OperatorFactory &#123; public static Operator createAddOperator()&#123; return new AddOperator(); &#125; public static Operator createSubOperator()&#123; return new SubOperator(); &#125; public static Operator createMulOperator()&#123; return new MulOperator(); &#125; public static Operator createDivOperator()&#123; return new DivOperator(); &#125;&#125; 此时不会出现简单工厂模式中因为字符串传错而不能正常创建对象的问题。 123456789101112public class OperatorTest &#123; public static void main(String[] args) &#123; Operator operator = OperatorFactory.createAddOperator(); operator.setNumberA(10); operator.setNumberB(5); try &#123; System.out.println(operator.getResult()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 总结：与简单工厂模式相比，工厂方法模式避免了因为传入字符串错误而导致无法正常创建对象的问题，并且由于多态的存在，客户端代码可以做到与特定应用无关，适用于任何实体类。缺点是每次增加一个产品时，都需要增加一个具体产品类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 3、抽象工厂模式定义：提供一个接口，用于创建 相关的对象家族 。 抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。 抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。 至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。 从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。 抽象工厂模式中包含的角色及职责： （1）抽象工厂角色（Creator） 这是抽象工厂模式的核心，任何工厂类必须实现这个接口。 （2）具体工厂角色（Concrete Creator） 它是抽象工厂的一个实现，负责实例化产品对象。 （3）抽象角色（Product） 抽象工厂模式所创建的所有对象的父类，它负责描述所有实例所共有的公共接口。 （4）具体产品角色（Concrete Product） 抽象工厂模式所创建的具体的实例对象。 下面结合实例理解一下： 我们将创建Shape和Color接口和实现这些接口的实现类，下一步是创建抽象工厂类AbastractFactory。接着定义工厂类ShapeFactory和ColorFactory，这两个工厂类都是扩展了AbstractFactory。然后创建一个工厂生成器FactoryProducer。 下面结合UML图理解一下： 根据上面的步骤，我们先为形状创建一个接口（抽象角色）： 123public interface Shape&#123; void draw();&#125; 然后创建接口的实体类（具体产品角色）： 123456public class Rectangle implements Shape&#123; @Override public void draw()&#123; System.out.println("矩形形状"); &#125;&#125; 123456public class Square implements Shape&#123; @Override public void draw()&#123; System.out.println("方形形状"); &#125;&#125; 123456public class Circle implements Shape&#123; @Override public void draw()&#123; System.out.println("园形形状"); &#125;&#125; 然后为颜色创建一个接口（抽象角色）： 123public interface Color&#123; void fill();&#125; 然后创建接口的实体类（具体产品角色）： 123456public class Red implements Color&#123; @Override public void fill()&#123; System.out.println("红色"); &#125;&#125; 123456public class Blue implements Color&#123; @Override public void fill()&#123; System.out.println("蓝色"); &#125;&#125; 123456public class Green implements Color&#123; @Override public void fill()&#123; System.out.println("绿色"); &#125;&#125; 然后我们继续创建抽象工厂角色和具体工厂角色： 先为Color和Shape对象创建抽象类来获取工厂： 1234public abstract class AbstractFactory&#123; public abstract Color getColor(String color); public abstract Shape getShape(String shape);&#125; 然后创建扩展了AbstractFactory的工厂类（具体工厂类），基于给定的信息生成实体类的对象。 12345678910111213141516171819202122public class ShapeFactory extends AbstractFactory&#123; @Override public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase("circle"))&#123; return new Circle(); &#125;esle if(shapeType.equalsIgnoreCase("rectangle"))&#123; return new Rectangle(); &#125;else if(shapeType.equalsIgnoreCase("square"))&#123; return new Square(); &#125; return null; &#125; @Override public Color getColor(String color)&#123; return null; &#125;&#125; 12345678910111213141516171819202122public class ColorFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType)&#123; return null; &#125; @Override public Color getColor(String color) &#123; if(color == null)&#123; return null; &#125; if(color.equalsIgnoreCase("red"))&#123; return new Red(); &#125; else if(color.equalsIgnoreCase("green"))&#123; return new Green(); &#125; else if(color.equalsIgnoreCase("blue"))&#123; return new Blue(); &#125; return null; &#125;&#125; 然后创建工厂生成器，通过传递形状或颜色信息来获取工厂。 12345678910public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice)&#123; if(choice.equalsIgnoreCase("shape"))&#123; return new ShapeFactory(); &#125; else if(choice.equalsIgnoreCase("color"))&#123; return new ColorFactory(); &#125; return null; &#125;&#125; 然后我们使用FactoryProducer来获取AbstractFactory，通过传递类型信息来获取实体类的对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class AbstractFactoryPatternDemo &#123; public static void main(String[] args) &#123; //获取形状工厂 AbstractFactory shapeFactory = FactoryProducer.getFactory("SHAPE"); //获取形状为 Circle 的对象 Shape shape1 = shapeFactory.getShape("CIRCLE"); //调用 Circle 的 draw 方法 shape1.draw(); //获取形状为 Rectangle 的对象 Shape shape2 = shapeFactory.getShape("RECTANGLE"); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取形状为 Square 的对象 Shape shape3 = shapeFactory.getShape("SQUARE"); //调用 Square 的 draw 方法 shape3.draw(); //获取颜色工厂 AbstractFactory colorFactory = FactoryProducer.getFactory("COLOR"); //获取颜色为 Red 的对象 Color color1 = colorFactory.getColor("RED"); //调用 Red 的 fill 方法 color1.fill(); //获取颜色为 Green 的对象 Color color2 = colorFactory.getColor("Green"); //调用 Green 的 fill 方法 color2.fill(); //获取颜色为 Blue 的对象 Color color3 = colorFactory.getColor("BLUE"); //调用 Blue 的 fill 方法 color3.fill(); &#125;&#125; 4、三种工厂模式的区别GOF在《设计模式》一书种将工厂模式分为两类：工厂方法模式（FactoryMethod）和抽象工厂模式（AbstractFactory）。 因此将简单工厂模式看为是工厂方法模式的一种特例，两者归为一类。 工厂方法模式 （1）一个抽象产品类，可以派生出多个具体产品类。 （2）一个抽象工厂类，可以派生出多个具体工厂类。 每个具体工厂类只能创建一个具体产品类的实例。 抽象工厂模式 （1）多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 （2）一个抽象工厂类，每个可以派生出多个具体工厂类。 （3）每个具体工厂类可以创建多个具体产品类的实例。 区别 （1）工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 （2）工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。 5、形象化总结三种工厂模式下面例子中鼠标，键盘，耳麦为产品，惠普，戴尔为工厂。 简单工厂模式简单工厂模式不是 23 种里的一种，简而言之，就是有一个专门生产某个产品的类。 比如下图中的鼠标工厂，专业生产鼠标，给参数 0，生产戴尔鼠标，给参数 1，生产惠普鼠标。 工厂模式工厂模式也就是鼠标工厂是个父类，有生产鼠标这个接口。 戴尔鼠标工厂，惠普鼠标工厂继承它，可以分别生产戴尔鼠标，惠普鼠标。 生产哪种鼠标不再由参数决定，而是创建鼠标工厂时，由戴尔鼠标工厂创建。 后续直接调用鼠标工厂.生产鼠标()即可 抽象工厂模式抽象工厂模式也就是不仅生产鼠标，同时生产键盘。 也就是 PC 厂商是个父类，有生产鼠标，生产键盘两个接口。 戴尔工厂，惠普工厂继承它，可以分别生产戴尔鼠标+戴尔键盘，和惠普鼠标+惠普键盘。 创建工厂时，由戴尔工厂创建。 后续工厂.生产鼠标()则生产戴尔鼠标，工厂.生产键盘()则生产戴尔键盘。 在抽象工厂中，假如我们要增加一个工厂： 假设我们增加华硕工厂，则我们需要增加华硕工厂，和戴尔工厂一样，继承 PC 厂商。 之后创建华硕鼠标，继承鼠标类。创建华硕键盘，继承键盘类即可。 在抽象工厂模式中，假如我们要增加一个产品： 假设我们增加耳麦这个产品，则首先我们需要增加耳麦这个父类，再加上戴尔耳麦，惠普耳麦这两个子类。 之后在PC厂商这个父类中，增加生产耳麦的接口。最后在戴尔工厂，惠普工厂这两个类中，分别实现生产戴尔耳麦，惠普耳麦的功能。 以上。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个设计模式：单例模式]]></title>
    <url>%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AA%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[一、单例模式在标准的23种设计模式种，单例设计模式在应用中是非常常见的，而我们在学习单例模式中，一定要考虑到和多线程结合起来时可能存在的各种问题以及其解决办法，这样我们才能写出一个在多线程环境下安全、正确的单例模式。 单例模式常见的有八种写法： 饿汉式（静态常量） 饿汉式（静态代码块） 懒汉式（线程不安全） 懒汉式（线程安全，同步方法） 懒汉式（线程安全，同步代码块） 双重检查锁 静态内部类 枚举 二、单例模式的基本实现思路：单例模式要求类能够有返回对象的一个引用（并且永远是同一个）和一个获得该实例的方法（必须是静态方法，通过使用getInstance这个名称） 单例模式的实现主要通过以下步骤： （1）将该类的构造方法定义为私有方法，这样其它的代码就无法通过调用该类的构造方法来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例； （2）在该类种提供一个静态方法，当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋值给该类保持的引用。 注意事项：单例模式在多线程的环境下必须小心使用，如果当唯一实例尚未创建时，有两个线程同时调用创建方法，那么它们同时没有检测到唯一实例的存在，从而同时各自创建了一个实例，这样就有两个实例被创建了出来，从而违反了单例模式种实例唯一的原则，解决的办法显而易见是加锁。 三、单例模式的八种写法1、饿汉式（静态常量）12345678910public class singleton()&#123; private final static Singleton INSTANCE = new Singleton(); //注意构造方法必须私有 private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return INSTANCE; &#125;&#125; 优点：写法简单，就是在类加载的时候完成实例化，避免了线程同步问题。 缺点：没有达到懒加载的效果，如果从始至终都未使用过这个实例，会造成内存的浪费。 2、饿汉式（静态代码块）12345678910111213public class Singleton&#123; private static Singleton instance; static&#123; instance = new Singleton(); &#125; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 这种方式跟第一种方式类似，都是在类加载的时候完成的，只不过将实例化的过程放在了静态代码块种，优缺点跟上面一样。 3、懒汉式（线程不安全）123456789101112public class Singleton&#123; private static Singleton instance; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(instance == null)&#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这种写法在单线程环境下可以使用，但是多线程环境下显然会产生多个实例。 4、懒汉式（线程安全，同步方法）123456789101112public class Singleton&#123; private static Singleton instance; private Singleton()&#123;&#125; public static synchronized Singleton getInstance()&#123; if(instance == null)&#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这种方法是对上面的线程不安全的懒汉式的改进。 由于每次去获取实例的时候都会进入synchronized代码块而不管实例是否为null，而其实这个方法只需要执行一次实例化代码就可以，因此这样的开销非常大，所以不推荐使用。 5、懒汉式（线程安全，同步代码块）1234567891011121314public class Singleton&#123; private static Singleton instance; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(instance == null)&#123; synchronized(Singleton.class)&#123; instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 并不能起到线程同步的作用，跟第三种方式遇到的情形一致。假如两个线程同时进入了if(instance == null)代码块，那么还是会产生多个实例，因此同样不推荐使用。 6、双重检查锁12345678910111213141516public class Singleton&#123; private static volatile Singleton singleton; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(singleton == null)&#123; synchronized(Singleton.class)&#123; if(singleton == null)&#123; singleton = new Singleton(): &#125; &#125; &#125; return singleton; &#125;&#125; 双重检查锁对于多线程开发者来说并不陌生，我们进行了两次if(singleton == null)判断，并通过将实例singleton设置为volatile变量，这样可以实现变量的可见性并且禁止编译器指令重排序造成的其它问题。关于双重检查锁的其它问题可以详见我的另外一篇博客Java中的双重检查锁。 优点：线程安全，延迟加载，效率较高。 7、静态内部类1234567891011public class Singleton&#123; private Singleton()&#123;&#125; private static class SingletonInstance&#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonInstance.INSTANCE; &#125;&#125; 这种方式跟饿汉式方式采用的机制类似，但又有不同。两者都是采用了类装载的机制来保证初始化实例时只有一个线程。不同的地方在饿汉式方式是只要Singleton类被装载就会实例化，没有Lazy-Loading的作用，而静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 优点：避免了线程不安全，延迟加载，效率高。 8、枚举123456public enum Singleton&#123; INSTANCE; public void whateverMethod()&#123; &#125;&#125; 借助JDK1.5中添加的枚举来实现单例模式。不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。可能是因为枚举在JDK1.5中才添加，所以在实际项目开发中，很少见人这么写过。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubantu系统常用操作指令]]></title>
    <url>%2FUbantu%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1、设置root用户密码刚装好的Ubantu系统是没有root用户密码的，因此我们在获取root权限首先需要设置root用户密码。 1$ sudo passwd 可以给初始系统设置root密码。 2、远程使用ssh登陆——安装ssh服务端Secure Shell（SSH）是一种加密网络协议，用于在不安全的网络上安全地运行网络服务。利用SSH可以实现加密并安全地远程登录计算机系统。 Ubantu安装好后默认只有ssh客户端，即只能在Ubantu内去连接其它ssh服务器，因此我们可以用指令去安装ssh服务端。 1$ sudo apt-get install openssh-server 注意：如果此处报错提示没有可供安装的候选者错误，可以更新apt-get 1$ sudo apt-get update 然后再使用上述命令更新。 编辑配置文件，让其支持root用户登陆，因为Ubantu默认是不支持ssh远程root登陆的。 1$ vim /etc/ssh/ssh_config 找到其中的PermitRootLogin prohibit-password修改为PermitRootLogin yes`。 重启openssh服务 1$ systemctl restart sshd.service 重启后会提示让你输入密码获取权限。 3、防火墙相关指令因为我们平常学习使用的时候不像生产开发那样只暴露某些端口，我们可以将Ubantu防火墙关闭，这样把所有端口都暴露出来方便我们操作。安装防火墙指令如下。 1$ sudo apt-get install ufw 使用方法： 1、启用 12345$ sudo ufw enable$ sudo ufw default deny作用：开启防火墙并随系统启动同时关闭所有外部对本机的访问（本机访问正常） 2、关闭 123$ sudo ufw disable$ sudo ufw stop 3、查看防火墙状态 1$ sudo ufw status 4、开启/禁用相应端口或服务举例 1234567891011$ sudo ufw allow 80 //允许外部访问80端口$ sudo ufw delete allow 80 //禁止外部访问80端口$ sudo ufw allow from 192.168.1.1 //允许此ip访问本机所有端口$ sudo ufw deny smtp //禁止外部访问smtp服务$ sudo ufw delete allow smtp //删除上面建立的某条规则$ sudo ufw deny proto tcp from 10.0.0.0/8 to 192.168.0.1 port 22 //要拒绝所有的TCP流量从10.0.0.0/8 到192.168.0.1地址的22端口 5、永久关闭防火墙 1$ chkconfig iptables off 4、打包/解压指令123456789$ tar -c 创建包 -x 释放包 -v 显示命令过程 -z 代表压缩包$ tar -cvf benet.tar /home/zuo/benet 将/home/benet目录打包$ tar -zcvf apache-tomcat7.tar.gz /home/zuo/tomcat 将/home/zuo/tomcat打包并压缩$ tar -zxvf apache-tomcat-7.0.93.tar.gz /home/zuo/ 将压缩包解压并放到指定目录下$ tar -jxvf apache-tomcat-7.0.93.tar.gz 解压缩 5、查看本机ip地址1$ ifconfig 6、文件权限设置12$ chmod u+x file 对文件file增加文件可执行权限$ chmod o-rwx 对文件file取消其它用户的所有权限]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Ubantu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP与HTTPS]]></title>
    <url>%2FHttp%E4%B8%8EHttps%2F</url>
    <content type="text"><![CDATA[一、HTTP的缺点 通信使用明文传输（不加密），内容可能会被窃听。 不验证通信方的身份，因此可能遭遇伪装。 无法证明报文的完整性，所以有可能已经遭遇篡改。 这些问题不仅在HTTP上出现，其它未经加密的协议种也会存在这类问题。除此之外，HTTP本身还有很多缺点，而且，还有像某些特定的Web服务器和特定的Web浏览器在实际应用种存在的不足（也可以说成是脆弱性或安全漏洞），另外，用Java和PHP等编程语言开发的Web应用也可能存在安全漏洞。 1、通信使用明文可能会被窃听由于HTTP本身不具备加密的功能，所以也无法做到对通信整体（使用HTTP协议通信的请求和响应的内容）进行加密，即：HTTP报文使用明文（指未经加密过的报文）方式发送。 （1）TCP/IP是可能被窃听的网络如果要问为什么通信时不加密是一个缺点，这是因为，按TCP/IP协议族的工作机制，通信内容在所有的通信线路上都有可能遭遇窥视。所谓互联网，是由能联通到全世界的网络组成的，无论世界上哪一个角落的服务器在和客户端进行通信时，在此通信线路上的某些网络设备、光缆、计算机等都不可能时个人的私有物，所以不排除某个环节遭到恶意窥视行为。 即使是已经加密处理的通信，也会被窥视到通信内容，这点和未加密的通信是相同的。只是说如果通信经过加密，就有可能让人无法看到破解报文信息的含义，但加密处理后的报文信息本身还是会被看到的。 窃听相同段上的通信并非难事，只需要收集在互联网上流动的数据包（帧）就可以。对于收集来的数据包的解析工作，可以交给那些抓包（Packet Capture）或嗅探器（Sniffer）工具。 （2）加密处理防止被窃听在目前大家正在研究的如何防止窃听保护信息的几种对策种，最普及的就是加密技术，加密的对象可以有这么几个。 1）通信的加密一种方式就是将通信加密。HTTP协议种没有加密机制，但可以通过和SSL（Secure Socket Layer，安全套接层）或TLS（Transport Layer Security，安全层传输协议）的组合使用，加密HTTP的通信内容。 用SSL建立安全通信线路后，就可以在这条线路上进行HTTP通信了。与SSL组合使用的HTTP被称为HTTPS（HTTP Secure，超文本传输安全协议）或HTTPS over SSL。 2）内容的加密还有一种将参与通信的内容本身加密的方式。由于HTTP协议种没有加密机制，那么就对HTTP协议传输的内容本身进行加密。即把HTTP报文里所含的内容跟进行加密处理。 在这种情况下，客户端要对HTTP报文进行加密处理后再发送请求。 诚然，为了做到有效的内容加密，前提是要求客户端和服务器同时具备加密和解密机制，主要应用在Web服务种。有一点必须引起注意，由于该方式不同于SSL或TLS将整个通信线路进行加密处理，所以内容仍然有被篡改的风险。 2、不验证通信方的身份就可能遭遇伪装HTTP协议种的请求和响应不会对通信方进行确认，也就是说存在“服务器是否就是发送请求种URI真正指定的主机，返回的响应是否真的返回到实际提出请求的客户端”等类似问题。 （1）任何人可以发起请求在HTTP协议通信时，由于不存在确认通信方的处理步骤，任何人都可以发起请求。另外，服务器只要接收到请求，不管对方是谁都会返回一个响应（但也仅限于发送端的IP地址和端口号没有被Web服务器设定限制访问的前提下）。 HTTP协议的实现本身非常简单，不论是谁发送过来的请求都会返回响应，因此不确认通信方，会存在以下各种隐患。 无法确定请求发送至目标的Web服务器是否是按照真实意图返回响应的那台服务器。有可能是已经伪装的Web服务器。 无法确定响应返回到的那台客户端是否是按真实意图接收响应的那个客户端。有可能是已经伪装的客户端。 无法确定正在通信的对方是否具备访问权限。因为某些Web服务器上保存着重要的信息，只想发给特定用户通信的权限。 无法判定请求是来自何方、出自谁手。 即使是无意义的请求也会照单全收。无法阻止海量请求下的Dos（Denial of Service，拒绝服务攻击）。 （2）查明对手的证书虽然使用HTTP协议无法确认通信方，但是如果使用SSL则可以，SSL不仅提供加密处理，而且还使用了一种被称为证书的手段，可用于确定方。 证书由值得信任的第三方机构颁发，用以证明服务器和客户端是实际存在的。另外，伪造证书从技术角度说是异常困难的一件事。所以只要能够确认通信方（服务器或客户端）持有的证书，即可判断通信方的真实意图。 通过使用证书，以证明通信方就是意料中的服务器，这对使用者个人来讲，也减少了个人信息泄露的危险性。 另外，客户端持有证书即可完成个人身份的确认，也可用于对Web网站的认证环节。 3、无法证明报文完整性，可能已遭篡改所谓完整性是指信息的准确度，若无法证明其完整性，通常也就意味着无法判断信息是否准确。 （1）接收到的内容可能有误由于HTTP协议无法证明通信报文的完整性，因此，在请求或响应送出之后直到对方接收之前的这段时间内，即使请求或响应的内容遭到篡改，也没有办法获悉。 换句话说，没有任何办法确认，发送的请求/响应和接收到的请求/响应是前后相同的。 比如，从某个 Web 网站上下载内容，是无法确定客户端下载文件和服务器上存放的文件是否前后一致的。文件内容在传输途中可能已经被篡改为其他的内容。即使内容真的已改变，作为接收方的客户端也是觉察不到的。像这样，请求或响应在传输途中，遭攻击者拦截并篡改内容的攻击称为中间人攻击（Man-in-the-Middle attack，MITM）。 （2）如何防止篡改虽然有使用 HTTP 协议确定报文完整性的方法，但事实上并不便捷、可靠。其中常用的是 MD5 和 SHA-1 等散列值校验的方法，以及用来确认文件的数字签名方法。 提供文件下载服务的 Web 网站也会提供相应的以 PGP（PrettyGood Privacy，完美隐私）创建的数字签名及 MD5 算法生成的散列值。PGP 是用来证明创建文件的数字签名，MD5 是由单向函数生成的散列值。不论使用哪一种方法，都需要操纵客户端的用户本人亲自检查验证下载的文件是否就是原来服务器上的文件。浏览器无法自动帮用户检查。可惜的是，用这些方法也依然无法百分百保证确认结果正确。因为 PGP 和 MD5 本身被改写的话，用户是没有办法意识到的。为了有效防止这些弊端，有必要使用 HTTPS。SSL提供认证和加密处理及摘要功能。仅靠 HTTP 确保完整性是非常困难的，因此通过和其他协议组合使用来实现这个目标。下节我们介绍HTTPS 的相关内容。 二、HTTP+ 加密 + 认证 + 完整性保护 = HTTPS如果在 HTTP 协议通信过程中使用未经加密的明文，比如在 Web 页面中输入信用卡号，如果这条通信线路遭到窃听，那么信用卡号就暴露了。另外，对于 HTTP 来说，服务器也好，客户端也好，都是没有办法确认通信方的。因为很有可能并不是和原本预想的通信方在实际通信。并且还需要考虑到接收到的报文在通信途中已经遭到篡改这一可能性。为了统一解决上述这些问题，需要在 HTTP 上再加入加密处理和认证等机制。我们把添加了加密及认证机制的 HTTP 称为 HTTPS（HTTPSecure）。 经常会在 Web 的登录页面和购物结算界面等使用 HTTPS 通信。使用HTTPS 通信时，不再用 http://，而是改用 https://。另外，当浏览器访问 HTTPS 通信有效的 Web 网站时，浏览器的地址栏内会出现一个带锁的标记。对 HTTPS 的显示方式会因浏览器的不同而有所改变。 1、HTTPS是身披外壳的HTTPHTTPS 并非是应用层的一种新协议。只是 HTTP 通信接口部分用SSL（Secure Socket Layer）和 TLS（Transport Layer Security）协议代替而已。 通常，HTTP 直接和 TCP 通信。当使用 SSL时，则演变成先和 SSL通信，再由 SSL和 TCP 通信了。简言之，所谓 HTTPS，其实就是身披SSL协议这层外壳的 HTTP。 在采用 SSL后，HTTP 就拥有了 HTTPS 的加密、证书和完整性保护这些功能。 SSL是独立于 HTTP 的协议，所以不光是 HTTP 协议，其他运行在应用层的 SMTP 和 Telnet 等协议均可配合 SSL协议使用。可以说 SSL是当今世界上应用最为广泛的网络安全技术。 2、HTTPS的加密（1）对称密钥加密对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。 （2）非对称密钥加密非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。 公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。 优点：可以更安全地将公开密钥传输给通信发送方； 缺点：运算速度慢。 （3）HTTPS采用的加密方式HTTPS 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。（下图中的 Session Key 就是对称密钥） 3、认证通过使用 证书 来对通信方进行认证。 数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。 服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。 进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。 4、完整性保护SSL 提供报文摘要功能来进行完整性保护。 HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。 HTTPS 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。 5、HTTPS的缺点 因为需要进行加密解密等过程，因此速度会更慢； 需要支付证书授权的高额费用。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot快速入门笔记]]></title>
    <url>%2FSpringBoot%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因注意是两点： 复杂的配置， 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 一个是混乱的依赖管理。 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这难题实在太棘手。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ Spring Boot 简化了基于Spring的应用开发，只需要“run”就能创建一个独立的、生产级别的Spring应用。Spring Boot为Spring平台及第三方库提供开箱即用的设置（提供默认设置，存放默认配置的包就是启动器），这样我们就可以简单的开始。多数Spring Boot应用只需要很少的Spring配置。 我们可以使用SpringBoot创建java应用，并使用java –jar 启动它，就能得到一个生产级别的web工程。 1. 了解SpringBoot在这一部分，我们主要了解以下2个问题： 什么是SpringBoot SpringBoot的特点 1.1.什么是SpringBootSpringBoot是Spring项目中的一个子工程，与我们所熟知的Spring-framework 同属于spring的产品: 我们可以看到下面的一段介绍： Takes an opinionated view of building production-ready Spring applications. Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible. 翻译一下： 用一些固定的方式来构建生产级别的spring应用。Spring Boot 推崇约定大于配置的方式以便于你能够尽可能快速的启动并运行程序。 其实人们把Spring Boot 称为搭建程序的脚手架。其最主要作用就是帮我们快速的构建庞大的spring项目，并且尽可能的减少一切xml配置，做到开箱即用，迅速上手，让我们关注与业务而非配置。 1.2.SpringBoot的特点Spring Boot 主要目标是： 为所有 Spring 的开发者提供一个非常快速的、广泛接受的入门体验 开箱即用（启动器starter-其实就是SpringBoot提供的一个jar包），但通过自己设置参数（.properties），即可快速摆脱这种方式。 提供了一些大型项目中常见的非功能性特性，如内嵌服务器、安全、指标，健康检测、外部化配置等 绝对没有代码生成，也无需 XML 配置。 更多细节，大家可以到官网查看。 2.快速入门接下来，我们就来利用SpringBoot搭建一个web工程，体会一下SpringBoot的魅力所在！ 2.1.创建工程我们先新建一个空的工程： 工程名为demo： 新建一个model： 使用maven来构建： 然后填写项目坐标： 目录结构： 项目结构： 2.2.添加依赖看到这里很多同学会有疑惑，前面说传统开发的问题之一就是依赖管理混乱，怎么这里我们还需要管理依赖呢？难道SpringBoot不帮我们管理吗？ 别着急，现在我们的项目与SpringBoot还没有什么关联。SpringBoot提供了一个名为spring-boot-starter-parent的工程，里面已经对各种常用依赖（并非全部）的版本进行了管理，我们的项目需要以这个项目为父工程，这样我们就不用操心依赖的版本问题了，需要什么依赖，直接引入坐标即可！ 2.2.1.添加父工程坐标12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 2.2.2.添加web启动器为了让SpringBoot帮我们完成各种自动配置，我们必须引入SpringBoot提供的自动配置依赖，我们称为启动器。因为我们是web项目，这里我们引入web启动器： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意的是，我们并没有在这里指定版本信息。因为SpringBoot的父工程已经对版本进行了管理了。 这个时候，我们会发现项目中多出了大量的依赖： 这些都是SpringBoot根据spring-boot-starter-web这个依赖自动引入的，而且所有的版本都已经管理好，不会出现冲突。 2.2.3.管理jdk版本默认情况下，maven工程的jdk版本是1.5，而我们开发使用的是1.8，因此这里我们需要修改jdk版本，只需要简单的添加以下属性即可： 123&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt; 2.2.4.完整pom123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leyou.demo&lt;/groupId&gt; &lt;artifactId&gt;springboot-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.3.启动类Spring Boot项目通过main函数即可启动，我们需要创建一个启动类： 然后编写main函数： 123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.4.编写controller接下来，我们就可以像以前那样开发SpringMVC的项目了！ 我们编写一个controller： 代码： 12345678@RestControllerpublic class HelloController &#123; @GetMapping("hello") public String hello()&#123; return "hello, spring boot!"; &#125;&#125; 2.5.启动测试接下来，我们运行main函数，查看控制台： 并且可以看到监听的端口信息： 1）监听的端口是8080 2）SpringMVC的映射路径是：/ 3）/hello路径已经映射到了HelloController中的hello()方法 打开页面访问：http://localhost:8080/hello 测试成功了！ 3.Java配置在入门案例中，我们没有任何的配置，就可以实现一个SpringMVC的项目了，快速、高效！ 但是有同学会有疑问，如果没有任何的xml，那么我们如果要配置一个Bean该怎么办？比如我们要配置一个数据库连接池，以前会这么玩： 1234567&lt;!-- 配置连接池 --&gt;&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt;&lt;/bean&gt; 现在该怎么做呢？ 3.1.回顾历史事实上，在Spring3.0开始，Spring官方就已经开始推荐使用java配置来代替传统的xml配置了，我们不妨来回顾一下Spring的历史： Spring1.0时代 在此时因为jdk1.5刚刚出来，注解开发并未盛行，因此一切Spring配置都是xml格式，想象一下所有的bean都用xml配置，细思极恐啊，心疼那个时候的程序员2秒 Spring2.0时代 Spring引入了注解开发，但是因为并不完善，因此并未完全替代xml，此时的程序员往往是把xml与注解进行结合，貌似我们之前都是这种方式。 Spring3.0及以后 3.0以后Spring的注解已经非常完善了，因此Spring推荐大家使用完全的java配置来代替以前的xml，不过似乎在国内并未推广盛行。然后当SpringBoot来临，人们才慢慢认识到java配置的优雅。 有句古话说的好：拥抱变化，拥抱未来。所以我们也应该顺应时代潮流，做时尚的弄潮儿，一起来学习下java配置的玩法。 3.2.尝试java配置java配置主要靠java类和一些注解，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @value：属性注入 @PropertySource：指定外部属性文件， 我们接下来用java配置来尝试实现连接池配置： 首先引入Druid连接池依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 创建一个jdbc.properties文件，编写jdbc属性： 1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/leyoujdbc.username=rootjdbc.password=123 然后编写代码： 1234567891011121314151617181920212223@Configuration@PropertySource("classpath:jdbc.properties")public class JdbcConfig &#123; @Value("$&#123;jdbc.url&#125;") String url; @Value("$&#123;jdbc.driverClassName&#125;") String driverClassName; @Value("$&#123;jdbc.username&#125;") String username; @Value("$&#123;jdbc.password&#125;") String password; @Bean public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125;&#125; 解读： @Configuration：声明我们JdbcConfig是一个配置类 @PropertySource：指定属性文件的路径是:classpath:jdbc.properties 通过@Value为属性注入值 通过@Bean将 dataSource()方法声明为一个注册Bean的方法，Spring会自动调用该方法，将方法的返回值加入Spring容器中。 然后我们就可以在任意位置通过@Autowired注入DataSource了！ 我们在HelloController中测试： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private DataSource dataSource; @GetMapping("hello") public String hello() &#123; return "hello, spring boot!" + dataSource; &#125;&#125; 然后Debug运行并查看： 属性注入成功了！ 3.3.SpringBoot的属性注入在上面的案例中，我们实验了java配置方式。不过属性注入使用的是@Value注解。这种方式虽然可行，但是不够强大，因为它只能注入基本类型值。 在SpringBoot中，提供了一种新的属性注入方式，支持各种java基本数据类型及复杂类型的注入。 1）我们新建一个类，用来进行属性注入： 123456789@ConfigurationProperties(prefix = "jdbc")public class JdbcProperties &#123; private String url; private String driverClassName; private String username; private String password; // ... 略 // getters 和 setters&#125; 在类上通过@ConfigurationProperties注解声明当前类为属性读取类 prefix=&quot;jdbc&quot;读取属性文件中，前缀为jdbc的值。 在类上定义各个属性，名称必须与属性文件中jdbc.后面部分一致 需要注意的是，这里我们并没有指定属性文件的地址，所以我们需要把jdbc.properties名称改为application.properties，这是SpringBoot默认读取的属性文件名： 2）在JdbcConfig中使用这个属性： 1234567891011121314@Configuration@EnableConfigurationProperties(JdbcProperties.class)public class JdbcConfig &#123; @Bean public DataSource dataSource(JdbcProperties jdbc) &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(jdbc.getUrl()); dataSource.setDriverClassName(jdbc.getDriverClassName()); dataSource.setUsername(jdbc.getUsername()); dataSource.setPassword(jdbc.getPassword()); return dataSource; &#125;&#125; 通过@EnableConfigurationProperties(JdbcProperties.class)来声明要使用JdbcProperties这个类的对象 然后你可以通过以下方式注入JdbcProperties： @Autowired注入 12@Autowiredprivate JdbcProperties prop; 构造函数注入 1234private JdbcProperties prop;public JdbcConfig(Jdbcproperties prop)&#123; this.prop = prop;&#125; 声明有@Bean的方法参数注入 1234@Beanpublic Datasource dataSource(JdbcProperties prop)&#123; // ...&#125; 本例中，我们采用第三种方式。 3）测试结果： 大家会觉得这种方式似乎更麻烦了，事实上这种方式有更强大的功能，也是SpringBoot推荐的注入方式。两者对比关系： 优势： Relaxed binding：松散绑定 不严格要求属性文件中的属性名与成员变量名一致。支持驼峰，中划线，下划线等等转换，甚至支持对象引导。比如：user.friend.name：代表的是user对象中的friend属性中的name属性，显然friend也是对象。@value注解就难以完成这样的注入方式。 meta-data support：元数据支持，帮助IDE生成属性提示（写开源框架会用到）。 ​ 3.4、更优雅的注入事实上，如果一段属性只有一个Bean需要使用，我们无需将其注入到一个类（JdbcProperties）中。而是直接在需要的地方声明即可： 1234567891011@Configurationpublic class JdbcConfig &#123; @Bean // 声明要注入的属性前缀，SpringBoot会自动把相关属性通过set方法注入到DataSource中 @ConfigurationProperties(prefix = "jdbc") public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); return dataSource; &#125;&#125; 我们直接把@ConfigurationProperties(prefix = &quot;jdbc&quot;)声明在需要使用的@Bean的方法上，然后SpringBoot就会自动调用这个Bean（此处是DataSource）的set方法，然后完成注入。使用的前提是：该类必须有对应属性的set方法！ 我们将jdbc的url改成：/heima，再次测试： 4.自动配置原理使用SpringBoot之后，一个整合了SpringMVC的WEB工程开发，变的无比简单，那些繁杂的配置都消失不见了，这是如何做到的？ 一切魔力的开始，都是从我们的main函数来的，所以我们再次来看下启动类： 我们发现特别的地方有两个： 注解：@SpringBootApplication run方法：SpringApplication.run() 我们分别来研究这两个部分。 4.1.了解@SpringBootApplication点击进入，查看源码： 这里重点的注解有3个： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 4.1.1.@SpringBootConfiguration我们继续点击查看源码： 通过这段我们可以看出，在这个注解上面，又有一个@Configuration注解。通过上面的注释阅读我们知道：这个注解的作用就是声明当前类是一个配置类，然后Spring会自动扫描到添加了@Configuration的类，并且读取其中的配置信息。而@SpringBootConfiguration是来声明当前类是SpringBoot应用的配置类，项目中只能有一个。所以一般我们无需自己添加。 4.1.2.@EnableAutoConfiguration关于这个注解，官网上有一段说明： The second class-level annotation is @EnableAutoConfiguration. This annotationtells Spring Boot to “guess” how you want to configure Spring, based on the jardependencies that you have added. Since spring-boot-starter-web added Tomcatand Spring MVC, the auto-configuration assumes that you are developing a webapplication and sets up Spring accordingly. 简单翻译以下： 第二级的注解@EnableAutoConfiguration，告诉SpringBoot基于你所添加的依赖，去“猜测”你想要如何配置Spring。比如我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖。此时自动配置就知道你是要开发一个web应用，所以就帮你完成了web及SpringMVC的默认配置了！ 总结，SpringBoot内部对大量的第三方库或Spring内部库进行了默认配置，这些配置是否生效，取决于我们是否引入了对应库所需的依赖，如果有那么默认配置就会生效。 所以，我们使用SpringBoot构建一个项目，只需要引入所需框架的依赖，配置就可以交给SpringBoot处理了。除非你不希望使用SpringBoot的默认配置，它也提供了自定义配置的入口。 4.1.3.@ComponentScan我们跟进源码： 并没有看到什么特殊的地方。我们查看注释： 大概的意思： 配置组件扫描的指令。提供了类似与&lt;context:component-scan&gt;标签的作用 通过basePackageClasses或者basePackages属性来指定要扫描的包。如果没有指定这些属性，那么将从声明这个注解的类所在的包开始，扫描包及子包 而我们的@SpringBootApplication注解声明的类就是main函数所在的启动类，因此扫描的包是该类所在包及其子包。因此，一般启动类会放在一个比较前的包目录中。 4.2.默认配置原理4.2.1默认配置类通过刚才的学习，我们知道@EnableAutoConfiguration会开启SpringBoot的自动配置，并且根据你引入的依赖来生效对应的默认配置。那么问题来了： 这些默认配置是在哪里定义的呢？ 为何依赖引入就会触发配置呢？ 其实在我们的项目中，已经引入了一个依赖：spring-boot-autoconfigure，其中定义了大量自动配置类： 还有： 非常多，几乎涵盖了现在主流的开源框架，例如： redis jms amqp jdbc jackson mongodb jpa solr elasticsearch … 等等 我们来看一个我们熟悉的，例如SpringMVC，查看mvc 的自动配置类： 打开WebMvcAutoConfiguration： 我们看到这个类上的4个注解： @Configuration：声明这个类是一个配置类 @ConditionalOnWebApplication(type = Type.SERVLET) ConditionalOn，翻译就是在某个条件下，此处就是满足项目的类是是Type.SERVLET类型，也就是一个普通web工程，显然我们就是 @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) 这里的条件是OnClass，也就是满足以下类存在：Servlet、DispatcherServlet、WebMvcConfigurer，其中Servlet只要引入了tomcat依赖自然会有，后两个需要引入SpringMVC才会有。这里就是判断你是否引入了相关依赖，引入依赖后该条件成立，当前类的配置才会生效！ @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 这个条件与上面不同，OnMissingBean，是说环境中没有指定的Bean这个才生效。其实这就是自定义配置的入口，也就是说，如果我们自己配置了一个WebMVCConfigurationSupport的类，那么这个默认配置就会失效！ 接着，我们查看该类中定义了什么： 视图解析器： 处理器适配器（HandlerAdapter）： 还有很多，这里就不一一截图了。 4.2.2.默认配置属性另外，这些默认配置的属性来自哪里呢？ 我们看到，这里通过@EnableAutoConfiguration注解引入了两个属性：WebMvcProperties和ResourceProperties。这不正是SpringBoot的属性注入玩法嘛。 我们查看这两个属性类： 找到了内部资源视图解析器的prefix和suffix属性。 ResourceProperties中主要定义了静态资源（.js,.html,.css等)的路径： 如果我们要覆盖这些默认属性，只需要在application.properties中定义与其前缀prefix和字段名一致的属性即可。 4.3.总结SpringBoot为我们提供了默认配置，而默认配置生效的条件一般有两个： 你引入了相关依赖 你自己没有配置 1）启动器 所以，我们如果不想配置，只需要引入依赖即可，而依赖版本我们也不用操心，因为只要引入了SpringBoot提供的stater（启动器），就会自动管理依赖及版本了。 因此，玩SpringBoot的第一件事情，就是找启动器，SpringBoot提供了大量的默认启动器，参考课前资料中提供的《SpringBoot启动器.txt》 2）全局配置 另外，SpringBoot的默认配置，都会读取默认属性，而这些属性可以通过自定义application.properties文件来进行覆盖。这样虽然使用的还是默认配置，但是配置中的值改成了我们自定义的。 因此，玩SpringBoot的第二件事情，就是通过application.properties来覆盖默认属性值，形成自定义配置。我们需要知道SpringBoot的默认属性key，非常多，参考课前资料提供的：《SpringBoot全局属性.md》 5.SpringBoot实践接下来，我们来看看如何用SpringBoot来玩转以前的SSM,我们沿用之前讲解SSM用到的数据库tb_user和实体类User 5.1.整合SpringMVC虽然默认配置已经可以使用SpringMVC了，不过我们有时候需要进行自定义配置。 5.1.1.修改端口查看SpringBoot的全局属性可知，端口通过以下方式配置： 12# 映射端口server.port=80 重启服务后测试： 5.1.2.访问静态资源现在，我们的项目是一个jar工程，那么就没有webapp，我们的静态资源该放哪里呢？ 回顾我们上面看的源码，有一个叫做ResourceProperties的类，里面就定义了静态资源的默认查找路径： 默认的静态资源路径为： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public 只要静态资源放在这些目录中任何一个，SpringMVC都会帮我们处理。 我们习惯会把静态资源放在classpath:/static/目录下。我们创建目录，并且添加一些静态资源： 重启项目后测试： 5.1.3.添加拦截器拦截器也是我们经常需要使用的，在SpringBoot中该如何配置呢？ 拦截器不是一个普通属性，而是一个类，所以就要用到java配置方式了。在SpringBoot官方文档中有这么一段说明： If you want to keep Spring Boot MVC features and you want to add additional MVC configuration (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, you can declare a WebMvcRegistrationsAdapter instance to provide such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 翻译： 如果你想要保持Spring Boot 的一些默认MVC特征，同时又想自定义一些MVC配置（包括：拦截器，格式化器, 视图控制器、消息转换器 等等），你应该让一个类实现WebMvcConfigurer，并且添加@Configuration注解，但是千万不要加@EnableWebMvc注解。如果你想要自定义HandlerMapping、HandlerAdapter、ExceptionResolver等组件，你可以创建一个WebMvcRegistrationsAdapter实例 来提供以上组件。 如果你想要完全自定义SpringMVC，不保留SpringBoot提供的一切特征，你可以自己定义类并且添加@Configuration注解和@EnableWebMvc注解 总结：通过实现WebMvcConfigurer并添加@Configuration注解来实现自定义部分SpringMvc配置。 首先我们定义一个拦截器： 12345678910111213141516171819public class LoginInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(LoginInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; logger.debug("preHandle method is now running!"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) &#123; logger.debug("postHandle method is now running!"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; logger.debug("afterCompletion method is now running!"); &#125;&#125; 然后，我们定义配置类，注册拦截器： 123456789101112131415161718192021@Configurationpublic class MvcConfig implements WebMvcConfigurer&#123; /** * 通过@Bean注解，将我们定义的拦截器注册到Spring容器 * @return */ @Bean public LoginInterceptor loginInterceptor()&#123; return new LoginInterceptor(); &#125; /** * 重写接口中的addInterceptors方法，添加自定义拦截器 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 通过registry来注册拦截器，通过addPathPatterns来添加拦截路径 registry.addInterceptor(this.loginInterceptor()).addPathPatterns("/**"); &#125;&#125; 结构如下： 接下来运行并查看日志： 你会发现日志中什么都没有，因为我们记录的log级别是debug，默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.*=debug来配置日志级别，*填写包名 12# 设置com.leyou包的日志级别为debuglogging.level.com.leyou=debug 再次运行查看： 1232018-05-05 17:50:01.811 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : preHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : postHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : afterCompletion method is now running! 5.2.整合jdbc和事务spring中的jdbc连接和事务是配置中的重要一环，在SpringBoot中该如何处理呢？ 答案是不需要处理，我们只要找到SpringBoot提供的启动器即可： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 当然，不要忘了数据库驱动，SpringBoot并不知道我们用的什么数据库，这里我们选择MySQL： 1234&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 至于事务，SpringBoot中通过注解来控制。就是我们熟知的@Transactional 123456789101112131415@Servicepublic class UserService &#123; @Autowired private UserMapper userMapper; public User queryById(Long id)&#123; return this.userMapper.selectByPrimaryKey(id); &#125; @Transactional public void deleteById(Long id)&#123; this.userMapper.deleteByPrimaryKey(id); &#125;&#125; 5.3.整合连接池其实，在刚才引入jdbc启动器的时候，SpringBoot已经自动帮我们引入了一个连接池： HikariCP应该是目前速度最快的连接池了，我们看看它与c3p0的对比： 因此，我们只需要指定连接池参数即可： 12345678910# 连接四大参数spring.datasource.url=jdbc:mysql://localhost:3306/heimaspring.datasource.username=rootspring.datasource.password=123# 可省略，SpringBoot自动推断spring.datasource.driverClassName=com.mysql.jdbc.Driverspring.datasource.hikari.idle-timeout=60000spring.datasource.hikari.maximum-pool-size=30spring.datasource.hikari.minimum-idle=10 当然，如果你更喜欢Druid连接池，也可以使用Druid官方提供的启动器： 123456&lt;!-- Druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 而连接信息的配置与上面是类似的，只不过在连接池特有属性上，方式略有不同： 12345678910#初始化连接数spring.datasource.druid.initial-size=1#最小空闲连接spring.datasource.druid.min-idle=1#最大活动连接spring.datasource.druid.max-active=20#获取连接时测试是否可用spring.datasource.druid.test-on-borrow=true#监控页面启动spring.datasource.druid.stat-view-servlet.allow=true 5.4.整合mybatis5.4.1.mybatisSpringBoot官方并没有提供Mybatis的启动器，不过Mybatis官网自己实现了： 123456&lt;!--mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 配置，基本没有需要配置的： 1234# mybatis 别名扫描mybatis.type-aliases-package=com.heima.pojo# mapper.xml文件位置,如果没有映射文件，请注释掉mybatis.mapper-locations=classpath:mappers/*.xml 需要注意，这里没有配置mapper接口扫描包，因此我们需要给每一个Mapper接口添加@Mapper注解，才能被识别。 123@Mapperpublic interface UserMapper &#123;&#125; 5.4.2.通用mapper通用Mapper的作者也为自己的插件编写了启动器，我们直接引入即可： 123456&lt;!-- 通用mapper --&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 不需要做任何配置就可以使用了。 123@Mapperpublic interface UserMapper extends tk.mybatis.mapper.common.Mapper&lt;User&gt;&#123;&#125; 5.5.启动测试将controller进行简单改造： 123456789101112@RestControllerpublic class HelloController &#123; @Autowired private UserService userService; @GetMapping("/hello") public User hello() &#123; User user = this.userService.queryById(8L); return user; &#125;&#125; 我们启动项目，查看： 6.Thymeleaf快速入门SpringBoot并不推荐使用jsp，但是支持一些模板引擎技术： 以前大家用的比较多的是Freemarker，但是我们今天的主角是Thymeleaf！ 6.1.为什么是Thymeleaf？简单说， Thymeleaf 是一个跟 Velocity、FreeMarker 类似的模板引擎，它可以完全替代 JSP 。相较与其他的模板引擎，它有如下三个极吸引人的特点： 动静结合：Thymeleaf 在有网络和无网络的环境下皆可运行，即它可以让美工在浏览器查看页面的静态效果，也可以让程序员在服务器查看带数据的动态页面效果。这是由于它支持 html 原型，然后在 html 标签里增加额外的属性来达到模板+数据的展示方式。浏览器解释 html 时会忽略未定义的标签属性，所以 thymeleaf 的模板可以静态地运行；当有数据返回到页面时，Thymeleaf 标签会动态地替换掉静态内容，使页面动态显示。 开箱即用：它提供标准和spring标准两种方言，可以直接套用模板实现JSTL、 OGNL表达式效果，避免每天套模板、该jstl、改标签的困扰。同时开发人员也可以扩展和创建自定义的方言。 多方言支持：Thymeleaf 提供spring标准方言和一个与 SpringMVC 完美集成的可选模块，可以快速的实现表单绑定、属性编辑器、国际化等功能。 与SpringBoot完美整合，SpringBoot提供了Thymeleaf的默认配置，并且为Thymeleaf设置了视图解析器，我们可以像以前操作jsp一样来操作Thymeleaf。代码几乎没有任何区别，就是在模板语法上有区别。 接下来，我们就通过入门案例来体会Thymeleaf的魅力： 6.2.编写接口编写一个controller，返回一些用户数据，放入模型中，等会在页面渲染 123456789@GetMapping("/all")public String all(ModelMap model) &#123; // 查询用户 List&lt;User&gt; users = this.userService.queryAll(); // 放入模型 model.addAttribute("users", users); // 返回模板名称（就是classpath:/templates/目录下的html文件名） return "users";&#125; 6.3.引入启动器直接引入启动器： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; SpringBoot会自动为Thymeleaf注册一个视图解析器： 与解析JSP的InternalViewResolver类似，Thymeleaf也会根据前缀和后缀来确定模板文件的位置： 默认前缀：classpath:/templates/ 默认后缀：.html 所以如果我们返回视图：users，会指向到 classpath:/templates/users.html 一般我们无需进行修改，默认即可。 6.4.静态页面根据上面的文档介绍，模板默认放在classpath下的templates文件夹，我们新建一个html文件放入其中： 编写html模板，渲染模型中的数据： 注意，把html 的名称空间，改成：xmlns:th=&quot;http://www.thymeleaf.org&quot; 会有语法提示 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;首页&lt;/title&gt; &lt;style type="text/css"&gt; table &#123;border-collapse: collapse; font-size: 14px; width: 80%; margin: auto&#125; table, th, td &#123;border: 1px solid darkslategray;padding: 10px&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div style="text-align: center"&gt; &lt;span style="color: darkslategray; font-size: 30px"&gt;欢迎光临！&lt;/span&gt; &lt;hr/&gt; &lt;table class="list"&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;用户名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;生日&lt;/th&gt; &lt;th&gt;备注&lt;/th&gt; &lt;/tr&gt; &lt;tr th:each="user : $&#123;users&#125;"&gt; &lt;td th:text="$&#123;user.id&#125;"&gt;1&lt;/td&gt; &lt;td th:text="$&#123;user.name&#125;"&gt;张三&lt;/td&gt; &lt;td th:text="$&#123;user.userName&#125;"&gt;zhangsan&lt;/td&gt; &lt;td th:text="$&#123;user.age&#125;"&gt;20&lt;/td&gt; &lt;td th:text="$&#123;user.sex&#125; == 1 ? '男': '女'"&gt;男&lt;/td&gt; &lt;td th:text="$&#123;#dates.format(user.birthday, 'yyyy-MM-dd')&#125;"&gt;1980-02-30&lt;/td&gt; &lt;td th:text="$&#123;user.note&#125;"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 我们看到这里使用了以下语法： ${} ：这个类似与el表达式，但其实是ognl的语法，比el表达式更加强大 th-指令：th-是利用了Html5中的自定义属性来实现的。如果不支持H5，可以用data-th-来代替 th:each：类似于c:foreach 遍历集合，但是语法更加简洁 th:text：声明标签中的文本 例如&lt;td th-text=&#39;${user.id}&#39;&gt;1&lt;/td&gt;，如果user.id有值，会覆盖默认的1 如果没有值，则会显示td中默认的1。这正是thymeleaf能够动静结合的原因，模板解析失败不影响页面的显示效果，因为会显示默认值！ 6.5.测试接下来，我们打开页面测试一下： 6.6.模板缓存Thymeleaf会在第一次对模板解析之后进行缓存，极大的提高了并发处理能力。但是这给我们开发带来了不便，修改页面后并不会立刻看到效果，我们开发阶段可以关掉缓存使用： 12# 开发阶段关闭thymeleaf的模板缓存spring.thymeleaf.cache=false 注意： ​ 在Idea中，我们需要在修改页面后按快捷键：Ctrl + Shift + F9 对项目进行rebuild才可以。 ​ eclipse中没有测试过。 我们可以修改页面，测试一下。]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker初级学习笔记]]></title>
    <url>%2FDocker%E5%88%9D%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一、简介Docker是一个开源的应用容器引擎，是一个轻量级容器技术；基于Go语言并遵循Apache2.0协议开源。 Docker可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口，更重要的是容器性能开销极低。 Docker支持将软件编译成一个镜像；然后在镜像中各种软件做好配置，将镜像发布出去，其他使用者可以直接使用这个镜像； 运行中的这个镜像称为容器，容器启动是非常快速的。类似Windows里面的ghost操作系统，安装好后就什么都有了。 二、核心概念docker主机(Host)：安装了Docker程序的机器（Docker直接安装在操作系统之上）； docker客户端(Client)：连接docker主机进行操作； docker仓库(Registry)：用来保存各种打包好的软件镜像； docker镜像(Images)：软件打包好的镜像；放在docker仓库中； docker容器(Container)：镜像启动后的实例称为一个容器；容器是独立运行的一个或一组应用 使用Docker的步骤： 1）、安装Docker 2）、去Docker仓库找到这个软件对应的镜像； 3）、使用Docker运行这个镜像，这个镜像就会生成一个Docker容器 4）、对容器的启动停止就是对软件的启动停止； 三、安装Docker1、安装linux虚拟机1）、VMWare、VirtualBox（安装）； 2）、导入虚拟机文件centos7-atguigu.ova； 3）、双击启动linux虚拟机;使用 root/ 123456登陆 4）、使用客户端连接linux服务器进行命令操作； 5）、设置虚拟机网络：桥接网络===选好网卡====接入网线； 6）、设置好网络以后使用命令重启虚拟机的网络 1service network restart 7）、查看linux的ip地址 1ip addr 2、在linux虚拟机上安装Docker步骤： 1、检查内核版本，必须是3.10及以上uname -r 2、安装dockeryum install docker 3、输入y确认安装 4、启动docker 123[root@localhost ~]# systemctl start docker[root@localhost ~]# docker -vDocker version 1.12.6, build 3e8e77d/1.12.6 5、开机启动docker 12[root@localhost ~]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. 6、停止docker 1systemctl stop docker 四、Docker常用命令&amp;操作1、镜像操作 操作 命令 说明 检索 docker search 关键字 eg：docker search redis 我们经常去docker hub上检索镜像的详细信息，如镜像的TAG。 拉取 docker pull 镜像名:tag :tag是可选的，tag表示标签，多为软件的版本，默认是latest 列表 docker images 查看所有本地镜像 删除 docker rmi image-id 删除指定的本地镜像 具体细节查阅：https://hub.docker.com/ 2、容器操作软件镜像（QQ安装程序）—-运行镜像—-产生一个容器（正在运行的软件，运行的QQ）； 步骤： 1、搜索镜像 1[root@localhost ~]# docker search tomcat 2、拉取镜像 1[root@localhost ~]# docker pull tomcat 3、根据镜像启动容器 1docker run --name mytomcat -d tomcat:latest 4、查看运行中的容器 1docker ps 5、 停止运行中的容器 1docker stop 容器的id 6、查看所有的容器 1docker ps -a 7、启动容器 1docker start 容器id 8、删除一个容器 1docker rm 容器id 9、启动一个做了端口映射的tomcat 1[root@localhost ~]# docker run -d -p 8888:8080 tomcat -d：后台运行 -p: 将主机的端口映射到容器的一个端口 主机端口:容器内部的端口 10、为了演示简单关闭了linux的防火墙 service firewalld status ：查看防火墙状态 service firewalld stop：关闭防火墙 11、查看容器的日志 1docker logs container-name/container-id 更多命令参看https://docs.docker.com/engine/reference/commandline/docker/可以参考每一个镜像的文档 3、安装MySQL示例1docker pull mysql 错误的启动，启动之后是用不了的 12345678910111213141516[root@localhost ~]# docker run --name mysql01 -d mysql42f09819908bb72dd99ae19e792e0a5d03c48638421fa64cce5f8ba0f40f5846mysql退出了[root@localhost ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES42f09819908b mysql "docker-entrypoint.sh" 34 seconds ago Exited (1) 33 seconds ago mysql01538bde63e500 tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago compassionate_goldstinec4f1ac60b3fc tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago lonely_fermi81ec743a5271 tomcat "catalina.sh run" About an hour ago Exited (143) About an hour ago sick_ramanujan//错误日志[root@localhost ~]# docker logs 42f09819908berror: database is uninitialized and password option is not specifiedYou need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD；这个三个参数必须指定一个 正确的启动 12345[root@localhost ~]# docker run --name mysql01 -e MYSQL_ROOT_PASSWORD=123456 -d mysqlb874c56bec49fb43024b3805ab51e9097da779f2f572c22c695305dedd684c5f[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb874c56bec49 mysql "docker-entrypoint.sh" 4 seconds ago Up 3 seconds 3306/tcp mysql01 做了端口映射 12345[root@localhost ~]# docker run -p 3306:3306 --name mysql02 -e MYSQL_ROOT_PASSWORD=123456 -d mysqlad10e4bc5c6a0f61cbad43898de71d366117d120e39db651844c0e73863b9434[root@localhost ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESad10e4bc5c6a mysql "docker-entrypoint.sh" 4 seconds ago Up 2 seconds 0.0.0.0:3306-&gt;3306/tcp mysql02 几个其它的高级操作 123456docker run --name mysql03 -v /conf/mysql:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag把主机的/conf/mysql文件夹挂载到 mysqldocker容器的/etc/mysql/conf.d文件夹里面改mysql的配置文件就只需要把mysql配置文件放在自定义的文件夹下（/conf/mysql）docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci指定mysql的一些配置参数]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记]]></title>
    <url>%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[一、NoSQL1.为什么用NoSQL? 单机MySQL的美好年代 Memcached（缓存）+MySQL+垂直拆分 MySQL主从读写分离 分表分库+水平拆分+MySQL集群 MySQL的扩展性瓶颈 ​ 在memcached的高速缓存,MySQL的主从复制，读写分离的基础上，这时MySQL主库的写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB引擎代替MyISAM。MySQL数据库也经常存储一些大文本字段，导致数据库表非常的大，在做数据库恢复的时候就导致非常的慢，不容易快速恢复数据库，比如1000w的4KB大小的文本就接近40GB的大小，如果能把这些数据从MySQL中省去，MySQL将变得非常得小，关系型数据库很强大，但是它并不能很好的适应所有的场景，MySQL的扩展性差（需要复杂的技术来实现），大数据下IO压力大，表结构更改困难，正是当前使用MySQL的开发人员面临的问题。 今天是什么样子？ 为什么用NoSQL？ 分库分表架构： 当前的架构： 2.NoSQL数据库中的CAP传统的ACID，而NoSQL中CAP： CAP的理论核心：一个分布式系统不可能同时很好的满足一致性、可用性和分区容错性三个需求。最多同时比较好的满足其中两个。 因此，根据CAP原理将NoSQL数据库分成了满足CA原则，满足CP原则和满足AP原则三类： CA - 单点集群，满足一致性、可用性的系统，通常在可扩展性上不太强大 CP - 满足一致性、分区容忍性的系统，通常性能不是特别高 AP - 满足可用性、分区容忍性的系统，通常可能怼一致性要求较低 CAP理论就是说在当前的分布式存储系统中。最多只能实现上面的两点，而由于当前的网络硬件肯定会出现延迟丢包等问题，因此，分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡，没有NoSQL系统能同时满足于三点。 CA：传统Orcale数据库 AP：大多数网站架构的选择 CP：Redis、Mongodb 3.BASEBASE就是为了解决关系数据库强一致性引起的问题而引起的可用性降低而提出的解决方案 基本可用 （Basically Avaliable） 软状态 （Soft state） 最终一致 (Eventually consistent) 它的思想是通过让系统放松对某一时刻数据一致性的要求来换取系统整体伸缩性和性能上改观。为什么这么说呢，缘由就在于大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成这些指标，要想获得这些指标，我们必须采用另外一种方式来完成，这里BASE就是解决这个问题的办法。 二、Redis1.Redis特点： Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次进行加载使用 Redis不仅仅支持简单的key-value类型的数据，同时还支持list、set、zset、hash等数据结构的存储 Redis支持数据的备份，即master-slave模式的数据备份 2.Redis能干嘛Redis能干嘛： 内存存储和持久化：redis支持异步将内存中的数据写到磁盘上，同时不影响继续服务 取最新N个数据的操作，如：可以将最新的10条评论的ID放在redis的List集合里面 模拟类似于HttpSession这种需要设定过期时间的功能 发布、订阅消息系统 定时器、计数器 3.Redis怎么玩Redis怎么玩： 1.数据类型、基本操作和配置 2.持久化和复制，RDB/AOF 3.事务的控制 ….. 4.具体知识 单进程：单进程模型来处理客户端的请求，对读写等事件的响应是通过对epoll函数的包装来做到的，Redis的实际处理速度完全依靠主进程的执行效率，epoll是Linux内核为处理大批量文件描述符而做了改进的epoll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。 默认16个数据库，类似数组下表从0开始，初始默认使用零号库 select 命令切换数据库 Dbsize查看当前数据库的key的数量 Flushdb：清空当前库 Flushall：通杀全部库 统一密码管理，16个库都是同样密码，要么都OK要么一个也连接不上 Redis索引都是从0开始 为什么默认端口是6379 5.Redis的数据类型 Redis五大数据类型 Redis键（key）和值（value） Redis字符串：是最基本的类型，可以理解为memcached一模一样的类型，一个key对应一个value，redis的string可以包含任何数据，比如jpg图片或者序列化对象 12341 redis 127.0.0.1:6379&gt; SET name &quot;hello&quot;2 OK3 redis 127.0.0.1:6379&gt; GET name4 &quot;hello&quot; Redis 列表（list） 简单的字符串列表，底层是个链表，按照插入顺序，你可以添加一个元素到列表的头部（左边）或者尾部（右边） 12345678910redis 127.0.0.1:6379&gt; lpush listtest test1(integer) 1redis 127.0.0.1:6379&gt; lpush listtest test2(integer) 2redis 127.0.0.1:6379&gt; lpush listtest test3(integer) 3redis 127.0.0.1:6379&gt; lrange listtest 0 -11 &quot;test1&quot;2 &quot;test2&quot;3 &quot;test3&quot; Redis 集合 （Set） string类型的无序集合，通过HashTable实现的 1234567redis 127.0.0.1:6379&gt; sadd setdemo set1(integer) 1redis 127.0.0.1:6379&gt; sadd setdemo set2(integer) 1redis 127.0.0.1:6379&gt; smembers setdemo1) &quot;set1&quot;2) &quot;set2&quot; Redis 哈希 （Hash）是一个键值对集合，是一个String类型的field和value的映射表，hash特别适合用于存储对象 12345redis 127.0.0.1:6379&gt; HMSET user:1 username testname password 123456OKredis 127.0.0.1:6379&gt; HGETALL user:11) &quot;testname&quot;2) &quot;123456&quot; Redis有序集合Zset（sorted set）string类型的集合并且无重复，不同的是每个元素都会关联一个double类型的分数，redis正是通过分数来为集合中的成员进行从小到大排序，zset的成员是唯一的，但是分数(score)是可以重复的 1234567redis 127.0.0.1:6379&gt; zadd list 0 name1(integer) 1redis 127.0.0.1:6379&gt; zadd list 0 name2(integer) 1redis 127.0.0.1:6379&gt; ZRANGEBYSCORE list 0 10001) &quot;name1&quot;2) &quot;name2&quot; 6.Redis的配置文件 配置大小写单位，开头定义了一些基本的度量单位，只支持bytes，不支持bite 对大小写不敏感 redis默认不是以守护进程的方式运行，可以通过配置修改该项，使用yes启用守护进程 当客户端闲置多长时间后关闭连接，设置timeout，默认为0表示永远不断开连接即关闭该功能 指定日志级别记录，redis支持四个级别，默认为verbose 指定redis的最大内存限制，redis在启动时会将数据加载到内存中，达到最大内存后，redis会尝试清楚已到期或者即将到期的key，当此方法处理后，仍然达到最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作，redis信的vm机制，会将key放进内存中，value放进swap区 默认为16个数据库 缓存过期清理策略，默认是永不过期，但是实际测试生产的时候肯定不会用默认配置 （1）volatile-lru：使用lru算法移除key，只对设置过期时间的建 （2）allkeys-lru：使用lru算法移除key （3）volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键 （4）allkeys-random：移除随机的key （5）volatile-ttl：移除那些ttl值最小的key，即那些最近要过期的key （6）noeviction：不进行移除，针对写操作，只是返回错误信息 设置样本数量，默认为5个 appendonly no指定是否在每次更新操作后进行日志记录，redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失，因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中，默认为No appendfilename指定更新日志文件名 指定更新日志条件共有三个值可选 7.Redis的持久化策略​ RDB：redis database ​ AOF：append only file （1）RDB在指定的时间间隔内将内存中的数据快照写入磁盘。也就是进行snapshot快照，它恢复时是按照快照直接读到内存里。 是什么​ redis会单独创建(fork)一个子进程来进行持久化，会先将数据写入到整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能，如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效，RDB的缺点是最后一次持久化的数据可能丢失。 Fork：fork的作用是复制一个与当前进程一样的进程，新进程的所有数据（变量、环境变量、程序计数器等）数据都和原进程一致，但是是一个全新的进程，并作为原进程的子进程 RDB保存的是dump.rdb文件，以下是redis.conf中RDB的配置文件： RDB是整个内存的压缩过的snapshot，RDB的数据结构，可以配置复合的快照触发条件，默认： 1分钟改了10000次 或5分钟改了10次 或15分钟改了1次 SNAPSHOTTING快照(1) save：save &lt;秒钟&gt; &lt;写操作次数&gt; 禁用 (2）stop-writes-on-bgsave-error：如果配置为no，表示你不在乎数据的一致性或者有其他的手段发现和控制 (3）rdbcompression：对于存储到磁盘中的快照，可以设置是否进行压缩存储，如果是的话，redis会采用，如果你想消耗cpu内存的话，可以关闭此功能 (4）rdbchecksum：在存储快照后，还可以让redis使用crc64算法来进行数据校验，但是这样会增加大约10%的性能消耗，如果希望得到最大的性能提升，可以关闭此功能 (5）dbfilename：dump.rdb (6) dir：文件位置 如何触发RDB快照：（1）配置文件中默认的快照配置：冷拷贝dump.rdb后重新使用 （2）命令save或者是bgsave ​ save：save只管保存，其他的不管 ​ bgsave：redis会在后台异步进行快照操作，快照同时还可以响应客户端请求，可以通过 lastsave命令获取最后一次成功执行快照的时间 （3）执行flushall命令，也会产生dump.rdb文件，但是里面是空的，没有意义 RDB的优势与劣势优势：适合大规模的数据恢复，对数据完整性和一致性要求不高 劣势：(1) 在一定时间间隔做一次备份，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改。 (2) Fork的时候，内存中的数据被clone了一份，大致2倍的数据膨胀需要考虑 如何停止：动态 停止RDB保存规则的方法：redis-cli config set save “” RDB小结（1）RDB是一个非常紧凑的文件 （2）RDB保存RDB文件时父进程唯一需要做的就是fork出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其它IO操作，所以RDB持久化方式可以最大化redis性能 （3）与AOF相比，在恢复大的数据集的时候，RDB方式会更快一些 （4）数据丢失风险更大 （5）RDB需要经常fork子进程来保存数据集到硬盘上，当数据集比较大的时候，fork的过程时非常耗时的，可能会导致redis在一些毫秒级不能响应客户端的请求。 （2）AOF是什么 以日志的形式来记录每个写操作，将redis执行过的所有写指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 redis-check-aof --fix appendonly.aof 可以执行修复AOF文件，将不符合AOF文件格式的都清除掉，可用于病毒、断电、丢包等情况。 参数设置appendonly: yes表示打开AOF策略 appendfilename：保存的文件名，一般保持默认即可 appendfsync： Always：同步持久化，每次发生数据变更时会被立即记录到磁盘，性能较差但数据完整性比较好 Everysec：出厂默认设置，异步操作，每秒记录 如果一秒内宕机，有数据丢失。 No no-appendfsync-on-write：重写时是否可以运用Appendfsync，用默认no即可，保证数据安全性 auto-aof-rewrite-min-size：设置重写的基准值 auto-aof-write-percentage：设置重写的基准值 AOF启动/恢复/修复正常恢复： 启动：设置yes，修改默认的appendonly no 改为yes 将有数据的aof文件复制一份保存到对应的目录（config get dir） 恢复：重启redis然后重新加载 异常恢复： 启动：设置yes 备份被写坏的AOF文件 修复：redis-check-aof –fix 进行修复 恢复：重启redis然后重新加载 Rewrite功能是什么：AOF采用文件追加方式，文件会越来越大为避免出现这种情况，新增了重写机制，当AOF文件的大小超过所设定的阈值的时候，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集，可以使用命令bgrewriteaof 重写原理：AOF文件持续增长而过大时，会fork出一条新进程来将文件重写（也是先写临时文件最后再rename），遍历新进程的内存中数据，每条记录会有一条set语句，重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容重写一个新的aof文件，这点和快照有点类似 触发机制：Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发 AOF的优势每秒同步：appendfsync always 同步持久化，每次发生数据变更时会被立即记录到磁盘，性能较差但数据完整性比较好 每修改同步：appendfsync everysec 异步操作，每秒记录 如果一秒宕机，有数据丢失 不同步：appendfsync no 从不同步 AOF的劣势 相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb AOF运行效率要慢于RDB，每秒同步策略效率较好，不同步效率和RDB相同 AOF小结 AOF文件是一个只进行追加的日志文件 Redis可以在AOF文件体积变得过大时，自动地在后台对AOF进行重写 AOF文件有序地保存了对数据库执行的所有写入操作，这些写入操作以redis协议的格式保存，因此AOF文件的内容非常容易被人读懂，对文件进行分析也很轻松 对于相同体积的数据集来说，AOF文件的体积通常要大于RDB文件的体积 根据所使用的fsync策略，AOF的速度可能会慢于RDB 性能建议 因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这一条规则。 如果Enable AOF,好处是在最恶劣的情况下也只会丢失不超过两秒的数据，启动脚本简单只Load自己的AOF文件就可以了，代价一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认为64M太小了，可以设置到5G以上，默认超过原大小100%大小时重写可以改到适当的数值。 如果不Enable AOF，仅靠Master-Slave Replication实现高可用也可以，能省掉一大笔IO也减少了rewrite时带来的系统波动，代价是如果Master/Slave同时down掉，会丢失十几分钟数据，启动脚本也要比较两个Master/Slave中的RDB文件，载入较新的那个，新浪微博就选用了这种架构 8.Redis的事务可以一次执行多个命令，本质是一组命令的集合。一个事务中的所有命令都会被序列化，按顺序地串行化执行命令而不会被其他命令插入，不许加塞。 一个队列中，一次性、顺序性、排他性的执行一系列命令 。 常用命令DISCARD、EXEC、MULTI、UNWATCH、WATCH key[key…] case1：正常执行 case2：放弃事务 case3：全体连坐 case4：冤头债主 case5：watch监控： 悲观锁/乐观锁/CAS (1) 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如Java里面的同步原语synchronized关键字的实现也是悲观锁。 (2) 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。CAS是乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 初始化信用卡可用余额和欠额 无加塞篡改，先监控再开启multi，保证两笔金额变动在同一个事务内 有加塞篡改 unwatch 一旦执行了exec之前加的监控锁都会被取消掉 小结 事务操作 放弃事务： 事务的三个特性 单独的隔离操作：事务中所有命令都会序列化、按顺序的执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断 没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前的任何指定都不会被实际执行，也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到“这个让人十分头痛的问题 不保证原子性：redis同一个事务中如果有一条命令执行失败，其后的所有命令仍然会被执行，没有回滚 9.Redis发布订阅是什么：进程间通信的一种消息通信模式：发送者（pub）发送消息，订阅者（sub）接收消息一般消息中间件不会用redis去做。 Redis自带的PUB/SUB机制，即发布-订阅模式。这种模式生产者(producer)和消费者(consumer)是1-M的关系，即一条消息会被多个消费者消费，当只有一个消费者时即可以看做一个1-1的消息队列，但这种方式并不适合题主的场景。首先，数据可靠性的无法保障，题主的数据最终需要落库，如果消息丢失、Redis宕机部分数据没有持久化甚至突然的网络抖动都可能带来数据的丢失，应该是无法忍受的。其次，扩展不灵活，没法通过多加consumer来加快消费的进度，如果前端写入数据太多，同步会比较慢，数据不同步的状态越久，风险越大，可以通过channel拆分的方式来解决，虽然不灵活，但可以规避。这种方案更适合于对数据可靠性要求不高，比如一些统计日志打点。 Redis的PUSH/POP机制，利用的Redis的列表(lists)数据结构。比较好的使用模式是，生产者lpush消息，消费者brpop消息，并设定超时时间，可以减少redis的压力。这种方案相对于第一种方案是数据可靠性提高了，只有在Redis宕机且数据没有持久化的情况下丢失数据，可以根据业务通过AOF和缩短持久化间隔来保证很高的可靠性，而且也可以通过多个client来提高消费速度。但相对于专业的消息队列来说，该方案消息的状态过于简单(没有状态)，且没有ack机制，消息取出后消费失败依赖于client记录日志或者重新push到队列里面。 10.Redis的主从复制主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主 主要是读写分离（主库可以写/读，从库只能读不能写）、容灾恢复 怎么玩1.配从（库）不配主（库） 2.从库配置：slaveof 主库ip 主库端口 (1) 每次与master断开之后，都需要重新连接，除非你配置进redis.conf文件 (2) info replication可以查看当前端口的角色 3.修改配置文件细节操作 4.常用三招： (1)一主二仆 (2)薪火相传：上一个slave可以是下一个slave的master，slave同样可以接收其他slaves的连接和同步请求，那么该slave作为了链条中下一个master，可以有效的减轻master的写压力 中途变更转向：会清除之前的数据，重新建立拷贝最新的。slaveof 新主库ip 新主库端口 (3)反客为主：主库挂了从库手动变成主库 slaveof no one 复制原理Slave启动成功连接到master后会发送一个sync命令，Master接到命令启动后台的存盘过程，同时收集所有接收到的用于修改数据命令，在后台进程执行完毕之后，master将传送的整个数据文件到slave，以完成一次完全同步。 全量复制：而slave的服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：master继续将新的所有收集到的修改命令依次传给slave，完成同步，但是只要是重新连接master，一次完全同步（全量复制）将被自动执行。 哨兵模式反客为主的自动版 能够后台监控主机是否故障，如果故障了根据投票数自动将从库换成主库使用步骤： （1）调整结构：6379带着6380、6381 （2）自定义的/myredis目录下新建sentinel.conf文件，名字绝对不能错。配置哨兵，填写内容：sentinel monitor 被监控数据库名字（自己起名字）127.0.0.1 6379 1 上面最后一个数字1，表示主机挂掉后slave投票看让谁替成主机，得票数多后成为主 （3）启动哨兵 （4）正常主从演示 （5）原来的Master挂了 （6）投票新选 （7）重新主从继续开工，info replication 查看 （8）问题：如果之前的master重启回来，会不会双Master冲突？不会，会成为新老大的slaver]]></content>
      <categories>
        <category>NoSQL</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JVM虚拟机（三）：类加载机制]]></title>
    <url>%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类是在运行期间第一次使用时动态加载的，而不是一次性加载。因为如果一次性加载，那么会占用很多的内存。 一、类的声明周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 二、类加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的二进制字节流。 将该字节流表示的静态存储结构转换为方法区的运行时存储结构。 在内存中生成一个代表该类的 Class 对象，作为方法区中该类各种数据的访问入口。 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从网络中获取，最典型的应用是 Applet。 运行时计算生成，例如动态代理技术，在 java.lang.reflect.Proxy 使用 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，例如由 JSP 文件生成对应的 Class 类。 2. 验证确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 3. 准备类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它会在对象实例化时随着对象一起被分配在堆中。应该注意到，实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次。 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。 1public static int value = 123; 如果类变量是常量，那么它将初始化为表达式所定义的值而不是 0。例如下面的常量 value 被初始化为 123 而不是 0。 1public static final int value = 123; 4. 解析将常量池的符号引用替换为直接引用的过程。 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 5. 初始化初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段是虚拟机执行类构造器 () 方法的过程。在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 在准备阶段，已经为类变量分配了系统所需的初始值，并且在初始化阶段，根据程序员通过程序进行的主观计划来初始化类变量和其他资源。 () 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 由于父类的 () 方法先执行，也就意味着父类中定义的静态语句块的执行要优先于子类。例如以下代码： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 2&#125; 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 () 方法。但接口与类不同的是，执行接口的 () 方法不需要先执行父接口的 () 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 () 方法。 虚拟机会保证一个类的 () 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 () 方法，其它线程都会阻塞等待，直到活动线程执行 () 方法完毕。如果在一个类的 () 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 三、类初始化时机1.主动引用虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 2.被动引用以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括： 通过子类引用父类的静态字段，不会导致子类初始化。 1System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 1SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 1System.out.println(ConstClass.HELLOWORLD); 四、类与类加载器两个类相等，需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 五、类加载分类从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），使用 C++ 实现，是虚拟机自身的一部分； 所有其它类的加载器，使用 Java 实现，独立于虚拟机，继承自抽象类 java.lang.ClassLoader。 从 Java 开发人员的角度看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JRE_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 六、双亲委派模型应用程序是由三种类加载器互相配合从而实现类加载，除此之外还可以加入自己定义的类加载器。 下图展示了类加载器之间的层次关系，称为双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其它的类加载器都要有自己的父类加载器。类加载器之间的父子关系一般通过组合关系（Composition）来实现，而不是继承关系（Inheritance）。 1. 工作过程一个类加载器首先将类加载请求转发到父类加载器，只有当父类加载器无法完成时才尝试自己加载。 2. 好处使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。 例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 并放到 ClassPath 中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，这是因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。rt.jar 中的 Object 优先级更高，那么程序中所有的 Object 都是这个 Object。 3. 实现以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class ClassLoader &#123; // The parent class loader for delegation private final ClassLoader parent; public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125;&#125; 七、自定义类加载器的实现FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。 java.lang.ClassLoader 的 loadClass() 实现了双亲委派模型的逻辑，自定义类加载器一般不去重写它，但是需要重写 findClass() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; &#125;&#125; 参考资料 周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Chapter 2. The Structure of the Java Virtual Machine Jvm memory Getting Started with the G1 Garbage Collector JNI Part1: Java Native Interface Introduction and “Hello World” application Memory Architecture Of JVM(Runtime Data Areas) JVM Run-Time Data Areas Android on x86: Java Native Interface and the Android Native Development Kit 深入理解 JVM(2)——GC 算法与内存分配策略 深入理解 JVM(3)——7 种垃圾收集器 JVM Internals 深入探讨 Java 类加载器 Guide to WeakHashMap in Java Tomcat example source code file (ConcurrentCache.java)]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JVM虚拟机（二）：垃圾收集、内存分配与回收策略]]></title>
    <url>%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[​ 垃圾回收主要是针对堆和方法区进行的。因为栈区域中的程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束后就会消失，因此不需要对这三个区域进行垃圾回收。 ​ 具体而言，栈中的栈帧随着方法的进入和退出而有条不紊的执行着入栈和出栈操作，每一个栈帧中分配多少内存基本上是在类结构确定下来的时候就已知的（尽管在运行期会由JIT编译器进行一些优化，但是大体上可以认为是编译器可知的）。因此这些区域的内存分配和回收都具有确定性。而Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序运行期间才能知道会创建哪些对象，这部分的内存分配和回收都是动态的，因此GC主要关注于堆和方法区。 一、判断一个对象是否可以回收1.引用计数法​ 为对象添加一个引用计数器，当对象增加一个引用时计数器加1，引用失效时计数器减1。引用计数为0时的对象可以被回收。 优点：实现简单，判定效率很高 缺点：无法解决对象间循环依赖问题。当两个对象出现循环依赖的情况下，此时引用计数器永远不为0，导致无法对它们进行回收。正是因为循环依赖问题的出现，因此Java虚拟机不使用引用计数法。 1234567891011121314public class Test &#123; public Object instance = null; public static void main(String[] args) &#123; Test a = new Test(); Test b = new Test(); a.instance = b; b.instance = a; //假设在这里发生GC,a和b能够被回收？ System.gc(); &#125;&#125; 2.可达性分析法以GC Roots为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。 在Java语言中，可作为GC Roots的对象包括以下几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 本地方法栈中JNI（即一般说的Native方法）引用的对象 3.方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代低很多，所以在方法区上进行回收性价比不高。 主要是对常量池的回收和对类的卸载。 为了避免内存溢出，在大量使用反射和动态代理的场景都需要虚拟机具备类卸载功能。 类的卸载条件很多，需要满足以下三个条件，并且满足了条件也不一定会被卸载： 该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 4.finalize()类似 C++ 的析构函数，用于关闭外部资源。但是 try-finally 等方式可以做得更好，并且该方法运行代价很高，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。 当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会再调用该方法。 二、引用类型无论是通过引用计数法判断对象的引用数量，还是通过可达性分析法判断对象是否可达，判定对象是否可以被回收都与引用有关。 Java提供了四种强度不同的引用类型。 1.强引用被强引用关联的对象不会被回收，一般是使用new创建一个新对象的方法来创建强引用 1Object obj = new Object(); 2.软引用软引用是用来描述一些还有用但非必须的对象。对于软引用关联着的对象，只有在内存不够的情况下才会被回收。 在JDK1.2后提供了SoftReference类来创建软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 3.弱引用弱引用也是来描述非必须对象的，但是它的强度比软引用更弱，被弱引用关联的对象只能存活到下一次GC之前。 JDK1.2后可以使用WeakReference来创建弱引用 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 4.虚引用又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。 为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知。 JDK1.2后可以使用 PhantomReference 来创建虚引用。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 三、垃圾回收算法1.标记-清除 标记要回收的对象，然后清除。 不足： 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2.标记-整理 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 3.复制算法 复制算法将将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法回收新生代，但是并不是划分为大小相等的两块，而是一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象全部复制到另一块 Survivor 上，最后清理 Eden 和使用过的那一块 Survivor。 HotSpot 虚拟机的 Eden 和 Survivor 大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 就不够用了，此时需要依赖于老年代进行空间分配担保，也就是借用老年代的空间存储放不下的对象。 4.分代收集现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法 四、垃圾收集器HotSpot虚拟机中的7个垃圾收集器： 连线表示垃圾收集器可以配和使用。 单线程与多线程：单线程指的是垃圾收集器只使用一个线程，而多线程使用多个线程； 串行与并行：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 1.Serial收集器 Serial 翻译为串行，也就是说它以串行的方式执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，在单个 CPU 环境下，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 场景下的默认新生代收集器，因为在该场景下内存一般来说不会很大。它收集一两百兆垃圾的停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿时间是可以接受的。 2.ParNew收集器 它是 Serial 收集器的多线程版本。 它是 Server 场景下默认的新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使用。 3.Parallel Scavenge收集器与 ParNew 一样是多线程收集器。 其它收集器目标是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户程序的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 4.Serial Old收集器 是 Serial 收集器的老年代版本，也是给 Client 场景下的虚拟机使用。如果用在 Server 场景下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5.Parallel Old收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6.CMS收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7.G1收集器G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。 G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 五、内存分配与回收策略1.Minor GC与Full GC Minor GC：指发生在新生代上的垃圾收集动作，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC（Major GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对，在Parallel Scavenge收集器的收集策略中就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 2.内存分配策略(1)对象优先在Eden分配大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 (2) 大对象直接进入老年代大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 ######(3) 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 (4) 动态对象年龄判定虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 (5) 空间分配担保在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那么就要进行一次 Full GC。 3.Full GC的触发条件对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： (1) 调用System.gc( )只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 (2) 老年代空间不足老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 ######(3) 空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第五小节。 (4) JDK1.7及以前的永久代空间不足在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 ######(5) Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的双重检查锁]]></title>
    <url>%2FJava%E4%B8%AD%E7%9A%84%E5%8F%8C%E9%87%8D%E6%A3%80%E6%9F%A5%E9%94%81%2F</url>
    <content type="text"><![CDATA[Java中的双重检查锁 Double Checked Locking在实现单例模式中，如果没有考虑多线程并发的情况下，初学者很容易写出下面的错误单例模式代码： 123456789101112public class Singleton()&#123; private static Singleton uniqueSingleton; private Singleton()&#123;&#125; private Singleton getInstance()&#123; if(null == uniqueSingleton)&#123; uniqueSingleton = new Singleton(); &#125; return uniqueSingleton; &#125;&#125; 很显然，在多线程的情况下，这样写可能会导致uniqueSingleton被创建出多个实例，比如下面的情况考虑有两个线程同时调用getInstance()方法时： Time Thread A 记为线程A Thread B 记为线程B T1 检查到uniqueSingleton为空 T2 检查到uniqueSingleton为空 T3 初始化对象A T4 返回对象A T5 初始化对象B T6 返回对象B 用文字表述如下： 如果线程A和B同时执行了getInstance()方法，然后以如下方式执行： 线程A进入if判断，此时instance为null，因此可以进入if内 线程B进入if判断，此时A还没有创建instance，因此instance也为null，因此线程B也进入了if内 线程B初始化了一个对象并返回 线程B也初始化了一个对象并返回 因此此时一个对象实际上还是可以被创建多次，并没有达到单例的效果。 ###加锁 那么出现这种情况，第一反应肯定是加锁，因此可以将上述代码更改如下： 12345678910111213public class Singleton()&#123; private static Singleton uniqueSingleton; private Singleton()&#123;&#125; //加锁 private synchronized Singleton getInstance()&#123; if(null == uniqueSingleton)&#123; uniqueSingleton = new Singleton(); &#125; return uniqueSingleton; &#125;&#125; 这样虽然解决了问题，但是因为用到了synchronized会导致比较大的开销，并且加锁实际中只需要在第一次初始化的时候用到，之后的调用并不需要再进行加锁，因此这种方法实际上需要改进。 双重检查锁双重检查锁是对上述加锁问题的一种优化，先判断对象是否已经被初始化，再决定是否加锁。 错误的双重检查锁12345678910111213141516public class Singleton&#123; private static Singleton uniqueSingleton; private SIngleton()&#123;&#125; public Singleton getInstance()&#123; if(null == uniqueSingleton)&#123; synchronized(Singleton.class)&#123; if(null == uniqueSingleton)&#123; uniqueSingleton = new Singleton(); //错误示范 &#125; &#125; &#125; return uniqueSingleton; &#125;&#125; 如果这样写的话，运行顺序就变成了： 1.检查变量是否被初始化（不去获得锁），如果已经被初始化就直接返回 2.获得锁 3.再次检查变量是否已经被初始化，如果还没有初始化就初始化一个对象 执行双重检查是因为，如果多个线程同时通过了第一次检查，并且其中的一个线程首先通过了第二次检查并实例化了对象，那么剩余通过了第一次检查的线程就不会再去实例化对象。 这样，除了初始化的时候会出现加锁的情况，后续的调用都会避免加锁而直接返回，解决了性能消耗的问题。 隐患上述的写法看似解决了问题，实际上有个很大的隐患。实例化对象的那一行代码，实际上可以分解为三个步骤： 分配内存空间 初始化对象 将对象指向刚分配的内存空间 但是有些编译器为了提升性能，会采用指令重排序，因此可能会将第二步和第三步进行重排序（在某些JIT编译器中这种情况是会真实发生的），顺序就变成了： 分配内存空间 将对象指向刚分配的内存空间 初始化对象 而当考虑重排序后，两个线程发生了以下的调用： Time Thread A 线程A Thread B 线程B T1 检查到uniqueSingleton为空 T2 获得锁 T3 再次检查到uniqueSingleton为空 T4 为uniqueSingleton分配内存空间 T5 将uniqueSingleton指向内存空间 T6 检查到uniqueSingleton不为空 T7 访问uniqueSingleton（此时对象还未完成初始化） T8 初始化uniqueSingleton 文字表述如下： （1） A、B线程同时进入了第一个if判断 （2） A首先初始化synchronized块，由于uniqueSingleton为null，所以它执行了uniqueSingleton = new Singleton(); （3） 由于JVM内部的优化机制，JVM先划出一些分配给Singleton实例的空白内存，并赋值给instance成员，注意此时JVM还没有开始初始化这个实例，然后A离开了这个synchronized块。 （4） B进入synchronized块，由于uniqueSingleton此时不是null，因此它马上离开了synchronized块 并将结果返回给调用该方法的程序。 （5） 此时B线程打算使用Singleton实例，却发现它没有被初始化，于是错误发生了。 在知晓了问题发生的根源之后，我们可以想出两种办法来实现线程安全的单例模式， 1）不允许2和3重排序。 2）允许2和3重排序，但是不允许其它线程“看到”这个重排序。 正确的双重检查锁1234567891011121314151617public class Singleton &#123; private volatile static Singleton uniqueSingleton; private Singleton() &#123; &#125; public Singleton getInstance() &#123; if (null == uniqueSingleton) &#123; synchronized (Singleton.class) &#123; if (null == uniqueSingleton) &#123; uniqueSingleton = new Singleton(); &#125; &#125; &#125; return uniqueSingleton; &#125;&#125; 为了解决隐患问题，我们需要在uniqueSingleton前加上关键字volatile。使用了volatile关键词后，编译器的指令重排序被禁止，所有对加上该关键字的共享变量的写（write）操作都发生在读（read）操作之前。至此，双重检查锁就可以完美工作了！ 基于类初始化的解决方案 JVM在类的初始化阶段（即在Class被加载后，且被线程使用之前），会执行类的 初始化。在执行类的初始化期间，JVM会尝试去获取一个锁。这个锁可以同步多个线程对同一个类的初始化。 基于这个特性，可以实现另外一种线程安全的延迟加载初始化方案： 123456789public class SingletonFactory&#123; private static class InstanceHolder&#123; public static Instance instance = new Instance(); &#125; public static Instance getInstance()&#123; return InstanceHolder.instance; //这里将导致InstanceHodler类被初始化 &#125;&#125; 这个方案的实质是：允许重排序，但是不允许非构造线程“看到”这个排序。]]></content>
      <categories>
        <category>Java</category>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java高并发</tag>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习模型之受限玻尔兹曼机(RBM)]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA-RBM%2F</url>
    <content type="text"><![CDATA[什么是受限玻尔兹曼机？​ 玻尔兹曼机是一大类的神经网络模型，但是在实际中运用的最多的还是RBM，RBM是一个随机神经网络（即当网络的神经元节点被激活时会有随机行为，随机取值）。它包含一层可视层和一层隐藏层，其中可视层的节点用v表示，隐藏层节点用h表示。在同一层的神经元之间是相互独立的，而在不同的网络层之间的神经元是相互连接的（双向连接）。在网络进行训练以及使用信息的时候会在两个方向进行流动，而且两个方向上的权值是相同的，但是偏置是不同的（偏置的个数与神经元的个数相同），其结构如图所示。 ​ ​ 我们可以看到上面一层神经元组成Hidden Layer，一般用h表示隐藏层神经元的值。下面一层神经元组成visiable layer，一般用v表示可见层神经元的值。连接权值我们用矩阵W表示，和DNN不同的是，RBM是不区分前向和后向的，可见层的状态可以作用于隐藏层，而隐藏层的状态可以作用于可见层。隐藏层的偏置系数是向量b，可见层的偏置系数是向量a。常见的RBM一般是二值的，即不管是隐藏层还是可见层，它们的神经元的取值是0或者1. RBM模型的结构：权重矩阵W，偏置系数a、b，隐藏神经元状态向量h和可见层神经元状态向量v ​ 如RBM这类由玻尔兹曼机发展而来的图模型都是基于能量的概率分布模型，分为两个部分：一部分是能量函数，第二部分是基于能量函数的概率分布函数。对于给定的状态向量h和v，则RBM当前的能量函数可以表示为如下 其中a和b是偏置系数，W是权值矩阵，有了能量函数，v和h的联合概率分布为： 其中Z被称为配分函数的归一化常数（对于概率输出一般要做归一化）： 由于配分函数Z的难以处理，所以必须使用最大似然梯度来近似，首先从联合分布中导出条件分布： 为了推导方便将无关值归于Z’中 可以看到就是相当于使用了sigmoid函数，现在可以写出关于隐藏层的完全分布条件： 有了激活函数，我们就可以从可见层和参数推导出隐藏层的神经元取值概率了，对于0，1取值的情况，则大于0.5取1，从隐藏层和参数导出可见的神经元的取值方法也是一样的。 RBM的损失函数​ RBM模型的关键就是求出我们模型中的参数W、a、b，首先我们得写出损失函数，RBM一般使用的是对数损失函数，即期望最小化： 然后对其求偏导： 虽然说梯度下降从理论上来说可以优化RBM模型，到那时实际中很难求得P(v)的概率分布的（P(v)表示可见层节点的联合概率）。计算复杂度非常大，因此采用一些随机采样的方法来得到近似的解。看这三个梯度的第二项实际上都是求期望，而我们知道，样本的均值是随机变量期望的无偏估计。因此一般都是基于对比散度方法来求解。 对比散度算法（CD-k算法） 深度受限玻尔兹曼机(DBM)加深RBM的层数后，就变成了DBM：结构图如下： 此时的能量函数变为： 联合概率变为： 其实DBM也可以看作是一个RBM，对上图稍加变换就可以看作是一个RBM： 将可见层和偶数隐藏层放在一边，将奇数隐藏层放在另一边，我们就得到了RBM，和RBM的细微区别只是现在的RBM并不是全连接的，其实也可以看做部分权重为0的全连接RBM。RBM的算法思想可以在DBM上使用。只是此时我们的模型参数更加的多，而且迭代求解参数也更加复杂了。 以下是Hinton教授给出的RBM经典二值化的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221from __future__ import print_functionimport numpy as npclass RBM: def __init__(self, num_visible, num_hidden): self.num_hidden = num_hidden self.num_visible = num_visible self.debug_print = True # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using # a uniform distribution between -sqrt(6. / (num_hidden + num_visible)) # and sqrt(6. / (num_hidden + num_visible)). One could vary the # standard deviation by multiplying the interval with appropriate value. # Here we initialize the weights with mean 0 and standard deviation 0.1. np_rng = np.random.RandomState(1234) self.weights = np.asarray(np_rng.uniform( low=-0.1 * np.sqrt(6. / (num_hidden + num_visible)), high=0.1 * np.sqrt(6. / (num_hidden + num_visible)), size=(num_visible, num_hidden))) # Insert weights for the bias units into the first row and first column. self.weights = np.insert(self.weights, 0, 0, axis=0) self.weights = np.insert(self.weights, 0, 0, axis=1) def train(self, data, max_epochs=1000, learning_rate=0.1): &quot;&quot;&quot; Train the machine. Parameters ---------- data: A matrix where each row is a training example consisting of the states of visible units. :param data: :param max_epochs: :param learning_rate: &quot;&quot;&quot; num_examples = data.shape[0] # Insert bias units of 1 into the first column. data = np.insert(data, 0, 1, axis=1) for epoch in range(max_epochs): # Clamp to the data and sample from the hidden units. # (This is the &quot;positive CD phase&quot;, aka the reality phase.) pos_hidden_activations = np.dot(data, self.weights) pos_hidden_probs = self._logistic(pos_hidden_activations) pos_hidden_probs[:, 0] = 1 # Fix the bias unit. pos_hidden_states = pos_hidden_probs &gt; np.random.rand(num_examples, self.num_hidden + 1) # Note that we&apos;re using the activation *probabilities* of the hidden states, not the hidden states # themselves, when computing associations. We could also use the states; see section 3 of Hinton&apos;s # &quot;A Practical Guide to Training Restricted Boltzmann Machines&quot; for more. pos_associations = np.dot(data.T, pos_hidden_probs) # Reconstruct the visible units and sample again from the hidden units. # (This is the &quot;negative CD phase&quot;, aka the daydreaming phase.) neg_visible_activations = np.dot(pos_hidden_states, self.weights.T) neg_visible_probs = self._logistic(neg_visible_activations) neg_visible_probs[:, 0] = 1 # Fix the bias unit. neg_hidden_activations = np.dot(neg_visible_probs, self.weights) neg_hidden_probs = self._logistic(neg_hidden_activations) # Note, again, that we&apos;re using the activation *probabilities* when computing associations, not the states # themselves. neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs) # Update weights self.weights += learning_rate * ((pos_associations - neg_associations) / num_examples) error = np.sum((data - neg_visible_probs) ** 2) if self.debug_print: print(&quot;Epoch %s: error is %s&quot; % (epoch, error)) def run_visible(self, data): &quot;&quot;&quot; Assuming the RBM has been trained (so that weights for the network have been learned), run the network on a set of visible units, to get a sample of the hidden units. Parameters ---------- data: A matrix where each row consists of the states of the visible units. Returns ------- hidden_states: A matrix where each row consists of the hidden units activated from the visible units in the data matrix passed in. &quot;&quot;&quot; num_examples = data.shape[0] # Create a matrix, where each row is to be the hidden units (plus a bias unit) # sampled from a training example. hidden_states = np.ones((num_examples, self.num_hidden + 1)) # Insert bias units of 1 into the first column of data. data = np.insert(data, 0, 1, axis=1) # Calculate the activations of the hidden units. hidden_activations = np.dot(data, self.weights) # Calculate the probabilities of turning the hidden units on. hidden_probs = self._logistic(hidden_activations) # Turn the hidden units on with their specified probabilities. hidden_states[:, :] = hidden_probs &gt; np.random.rand(num_examples, self.num_hidden + 1) # Always fix the bias unit to 1. # hidden_states[:,0] = 1 # Ignore the bias units. hidden_states = hidden_states[:, 1:] return hidden_states # TODO: Remove the code duplication between this method and `run_visible`? def run_hidden(self, data): &quot;&quot;&quot; Assuming the RBM has been trained (so that weights for the network have been learned), run the network on a set of hidden units, to get a sample of the visible units. Parameters ---------- data: A matrix where each row consists of the states of the hidden units. Returns ------- visible_states: A matrix where each row consists of the visible units activated from the hidden units in the data matrix passed in. &quot;&quot;&quot; num_examples = data.shape[0] # Create a matrix, where each row is to be the visible units (plus a bias unit) # sampled from a training example. visible_states = np.ones((num_examples, self.num_visible + 1)) # Insert bias units of 1 into the first column of data. data = np.insert(data, 0, 1, axis=1) # Calculate the activations of the visible units. visible_activations = np.dot(data, self.weights.T) # Calculate the probabilities of turning the visible units on. visible_probs = self._logistic(visible_activations) # Turn the visible units on with their specified probabilities. visible_states[:, :] = visible_probs &gt; np.random.rand(num_examples, self.num_visible + 1) # Always fix the bias unit to 1. # visible_states[:,0] = 1 # Ignore the bias units. visible_states = visible_states[:, 1:] return visible_states def daydream(self, num_samples): &quot;&quot;&quot; Randomly initialize the visible units once, and start running alternating Gibbs sampling steps (where each step consists of updating all the hidden units, and then updating all of the visible units), taking a sample of the visible units at each step. Note that we only initialize the network *once*, so these samples are correlated. Returns ------- samples: A matrix, where each row is a sample of the visible units produced while the network was daydreaming. &quot;&quot;&quot; # Create a matrix, where each row is to be a sample of of the visible units # (with an extra bias unit), initialized to all ones. samples = np.ones((num_samples, self.num_visible + 1)) # Take the first sample from a uniform distribution. samples[0, 1:] = np.random.rand(self.num_visible) # Start the alternating Gibbs sampling. # Note that we keep the hidden units binary states, but leave the # visible units as real probabilities. See section 3 of Hinton&apos;s # &quot;A Practical Guide to Training Restricted Boltzmann Machines&quot; # for more on why. for i in range(1, num_samples): visible = samples[i - 1, :] # Calculate the activations of the hidden units. hidden_activations = np.dot(visible, self.weights) # Calculate the probabilities of turning the hidden units on. hidden_probs = self._logistic(hidden_activations) # Turn the hidden units on with their specified probabilities. hidden_states = hidden_probs &gt; np.random.rand(self.num_hidden + 1) # Always fix the bias unit to 1. hidden_states[0] = 1 # Recalculate the probabilities that the visible units are on. visible_activations = np.dot(hidden_states, self.weights.T) visible_probs = self._logistic(visible_activations) visible_states = visible_probs &gt; np.random.rand(self.num_visible + 1) samples[i, :] = visible_states # Ignore the bias units (the first column), since they&apos;re always set to 1. return samples[:, 1:] @staticmethod def _logistic(x): return 1.0 / (1 + np.exp(-x))&apos;&apos;&apos;import some fake datas to validate the modelAlice: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan（科幻超级迷妹）.Bob: (Harry Potter = 1, Avatar = 0, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). SF/fantasy fan, but doesn’t like Avatar（科幻迷，但不喜欢阿凡达）.Carol: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan（科幻超级迷）.David: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan（奥斯卡超级迷）.Eric: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 0, Glitter = 0). Oscar winners fan, except for Titanic（奥斯卡迷，但不喜欢泰坦尼克号（译者注：原文此处Titanic = 1有误））.Fred: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan（奥斯卡超级迷）.&apos;&apos;&apos;if __name__ == &apos;__main__&apos;: r = RBM(num_visible=6, num_hidden=2) # training_data = np.array(readData(&apos;datas/0-201812180801_330.txt&apos;)) # print(training_data.shape) training_data = np.array( [[1, 1, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 0, 0], [0, 0, 1, 1, 1, 0]]) r.train(training_data, max_epochs=5000) print(r.weights) user = np.array([[0, 0, 0, 1, 1, 0]]) print(r.run_visible(user))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>RBM</tag>
        <tag>受限玻尔兹曼机</tag>
        <tag>概率图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的容器]]></title>
    <url>%2FJava%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、概览容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。 Collection 1. Set TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。 HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。 LinkedHashSet：具有 HashSet 的查找效率，且内部使用双向链表维护元素的插入顺序。 2. List ArrayList：基于动态数组实现，支持随机访问。 Vector：和 ArrayList 类似，但它是线程安全的。 LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 3. Queue LinkedList：可以用它来实现双向队列。 PriorityQueue：基于堆结构实现，可以用它来实现优先队列。 Map TreeMap：基于红黑树实现。 HashMap：基于哈希表实现。 HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。它是遗留类，不应该去使用它。现在可以使用 ConcurrentHashMap 来支持线程安全，并且 ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。 LinkedHashMap：使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。 二、容器中的设计模式迭代器模式 Collection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 123456List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("a");list.add("b");for (String item : list) &#123; System.out.println(item);&#125; 适配器模式java.util.Arrays#asList() 可以把数组类型转换为 List 类型。 12@SafeVarargspublic static &lt;T&gt; List&lt;T&gt; asList(T... a) 应该注意的是 asList() 的参数为泛型的变长参数，不能使用基本类型数组作为参数，只能使用相应的包装类型数组。 12Integer[] arr = &#123;1, 2, 3&#125;;List list = Arrays.asList(arr); 也可以使用以下方式调用 asList()： 1List list = Arrays.asList(1, 2, 3); 三、源码分析如果没有特别说明，以下源码分析基于 JDK 1.8。 在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。 ArrayList1. 概览实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; 2. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 3. 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看出 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 4. Fail-FastmodCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。 123456789101112131415161718private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 5. 序列化ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。 保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。 1transient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 123456789101112131415161718192021private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 123456789101112131415161718private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。 123ArrayList list = new ArrayList();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));oos.writeObject(list); Vector1. 同步它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; 2. 与 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 3. 替代方案可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。 写操作需要加锁，防止并发写入时导致写入数据丢失。 写操作结束之后需要把原始数组指向新的复制数组。 123456789101112131415161718public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 1234@SuppressWarnings("unchecked")private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 适用场景CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷： 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 LinkedList1. 概览基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 first 和 last 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 2. 与 ArrayList 的比较 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。 HashMap为了便于理解，以下源码分析以 JDK 1.7 为主。 1. 存储结构内部包含了一个 Entry 类型的数组 table。 1transient Entry[] table; Entry 存储着键值对。它包含了四个字段，从 next 字段我们可以看出 Entry 是一个链表。即数组中的每个位置被当成一个桶，一个桶存放一个链表。HashMap 使用拉链法来解决冲突，同一个链表中存放哈希值相同的 Entry。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; public final int hashCode() &#123; return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125;&#125; 2. 拉链法的工作原理1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put("K1", "V1");map.put("K2", "V2");map.put("K3", "V3"); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16=3。 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6。 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6，插在 &lt;K2,V2&gt; 前面。 应该注意到链表的插入是以头插法方式进行的，例如上面的 &lt;K3,V3&gt; 不是插在 &lt;K2,V2&gt; 后面，而是插入在链表头部。 查找需要分成两步进行： 计算键值对所在的桶； 在链表上顺序查找，时间复杂度显然和链表的长度成正比。 3. put 操作1234567891011121314151617181920212223242526public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 键为 null 单独处理 if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标 int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入新键值对 addEntry(hash, key, value, i); return null;&#125; HashMap 允许插入键为 null 的键值对。但是因为无法调用 null 的 hashCode() 方法，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。 12345678910111213private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null;&#125; 使用链表的头插法，也就是新的键值对插在链表的头部，而不是链表的尾部。 12345678910111213141516void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; // 头插法，链表头部指向新的键值对 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 123456Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h;&#125; 4. 确定桶下标很多操作都需要先确定一个键值对所在的桶下标。 12int hash = hash(key);int i = indexFor(hash, table.length); 4.1 计算 hash 值 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 123public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value);&#125; 4.2 取模 令 x = 1&lt;&lt;4，即 x 为 2 的 4 次方，它具有以下性质： 12x : 00010000x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数： 123y : 10110010x-1 : 00001111y&amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的： 123y : 10110010x : 00010000y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。 确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。 123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 5. 扩容-基本原理设 HashMap 的 table 长度为 M，需要存储的键值对数量为 N，如果哈希函数满足均匀性的要求，那么每条链表的长度大约为 N/M，因此平均查找次数的复杂度为 O(N/M)。 为了让查找的成本降低，应该尽可能使得 N/M 尽可能小，因此需要保证 M 尽可能大，也就是说 table 要尽可能大。HashMap 采用动态扩容来根据当前的 N 值来调整 M 值，使得空间效率和时间效率都能得到保证。 和扩容相关的参数主要有：capacity、size、threshold 和 load_factor。 参数 含义 capacity table 的容量大小，默认为 16。需要注意的是 capacity 必须保证为 2 的 n 次方。 size 键值对数量。 threshold size 的临界值，当 size 大于等于 threshold 就必须进行扩容操作。 loadFactor 装载因子，table 能够使用的比例，threshold = capacity * loadFactor。 123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size;int threshold;final float loadFactor;transient int modCount; 从下面的添加元素代码中可以看出，当需要扩容时，令 capacity 为原来的两倍。 123456void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);&#125; 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步是很费时的。 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 6. 扩容-重新计算桶下标在进行扩容时，需要把键值对重新放到对应的桶上。HashMap 使用了一个特殊的机制，可以降低重新计算桶下标的操作。 假设原数组长度 capacity 为 16，扩容之后 new capacity 为 32： 12capacity : 00010000new capacity : 00100000 对于一个 Key， 它的哈希值如果在第 5 位上为 0，那么取模得到的结果和之前一样； 如果为 1，那么得到的结果为原来的结果 +16。 7. 计算数组容量HashMap 构造函数允许用户传入的容量不是 2 的 n 次方，因为它可以自动地将传入的容量转换为 2 的 n 次方。 先考虑如何求一个数的掩码，对于 10010000，它的掩码为 11111111，可以使用以下方法得到： 123mask |= mask &gt;&gt; 1 11011000mask |= mask &gt;&gt; 2 11111110mask |= mask &gt;&gt; 4 11111111 mask+1 是大于原始数字的最小的 2 的 n 次方。 12num 10010000mask+1 100000000 以下是 HashMap 中计算数组容量的代码： 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 8. 链表转红黑树从 JDK 1.8 开始，一个桶存储的链表长度大于 8 时会将链表转换为红黑树。 9. 与 HashTable 的比较 HashTable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 ConcurrentHashMap1. 存储结构123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 Segment 继承自 ReentrantLock。 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。 12345/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 ConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。 尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。 如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */static final int RETRIES_BEFORE_LOCK = 2;public int size() &#123; // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; // 超过尝试次数，则对每个 Segment 加锁 if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 3. JDK 1.8 的改动JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发度与 Segment 数量相等。 JDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。 并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。 LinkedHashMap存储结构继承自 HashMap，因此具有和 HashMap 一样的快速查找特性。 1public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。 123456789/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail; accessOrder 决定了顺序，默认为 false，此时维护的是插入顺序。 1final boolean accessOrder; LinkedHashMap 最重要的是以下用于维护顺序的函数，它们会在 put、get 等方法中调用。 12void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125; afterNodeAccess()当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时，会将这个节点移到链表尾部，保证链表尾部是最近访问的节点，那么链表首部就是最近最久未使用的节点。 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; afterNodeInsertion()在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。 evict 只有在构建 Map 的时候才为 false，在这里为 true。 1234567void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，这在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; LRU 缓存以下是使用 LinkedHashMap 实现的一个 LRU 缓存： 设定最大缓存空间 MAX_ENTRIES 为 3； 使用 LinkedHashMap 的构造函数将 accessOrder 设置为 true，开启 LRU 顺序； 覆盖 removeEldestEntry() 方法实现，在节点多于 MAX_ENTRIES 就会将最近最久未使用的数据移除。 1234567891011class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private static final int MAX_ENTRIES = 3; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125; LRUCache() &#123; super(MAX_ENTRIES, 0.75f, true); &#125;&#125; 123456789public static void main(String[] args) &#123; LRUCache&lt;Integer, String&gt; cache = new LRUCache&lt;&gt;(); cache.put(1, "a"); cache.put(2, "b"); cache.put(3, "c"); cache.get(1); cache.put(4, "d"); System.out.println(cache.keySet());&#125; 1[3, 1, 4] WeakHashMap存储结构WeakHashMap 的 Entry 继承自 WeakReference，被 WeakReference 关联的对象在下一次垃圾回收时会被回收。 WeakHashMap 主要用来实现缓存，通过使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; ConcurrentCacheTomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能。 ConcurrentCache 采取的是分代缓存： 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）； 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收。 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象。 1234567891011121314151617181920212223242526272829303132public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; v = this.longterm.get(k); if (v != null) this.eden.put(k, v); &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; this.longterm.putAll(this.eden); this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; 参考资料 Eckel B. Java 编程思想 [M]. 机械工业出版社, 2002. Java Collection Framework Iterator 模式 Java 8 系列之重新认识 HashMap What is difference between HashMap and Hashtable in Java? Java 集合之 HashMap The principle of ConcurrentHashMap analysis 探索 ConcurrentHashMap 高并发性的实现机制 HashMap 相关面试题及其解答 Java 集合细节（二）：asList 的缺陷 Java Collection Framework – The LinkedList Class]]></content>
      <categories>
        <category>Java</category>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java面试</tag>
        <tag>JavaSE</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JVM虚拟机（一）：Java运行时数据区域]]></title>
    <url>%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-%E4%B8%80-%EF%BC%9AJava%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[Java运行时数据区域分析Java运行时数据区域结构模型如下： ​ 我们可以看到，JVM运行时数据区域大致可以分为：程序计数器、Java虚拟机栈、本地方法栈、Java堆、方法区、运行时常量池、直接内存等区域。在未具体了解JVM运行时内存区域之前，大多数程序员都会普遍的认为其中可以分为栈区域和堆区域两种，而这种理解比较浅显，也可以认为这其中的栈区域就是指上述的Java虚拟机栈，堆区域就是指堆（堆区域中实际包括不只堆）。 ###1、程序计数器 程序计数器（Program Counter Register）可以看作是当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。每个线程都需要有一个独立的程序计数器，因此PC是“线程私有”的。并且这个计数器记录的是正在执行的虚拟机字节码指令的地址（如果是Native本地方法，那么计数器值为Undifined）。 2、Java虚拟机栈 与程序计数器一样，Java虚拟机栈也是线程私有的，并且其生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型，每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 可以通过 -Xss 这个虚拟机参数来指定每个线程的 Java 虚拟机栈内存大小： 1java -Xss512M HackTheJava 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 3、本地方法栈 本地方法栈与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。 本地方法一般是用其它语言（C、C++ 或汇编语言等）编写的，并且被编译为基于本机硬件和操作系统的程序，对待这些方法需要特别处理。 与Java虚拟机栈一样，本地方法栈区域也会抛出StackOverFlow和OutOfMemoryError异常。 4、堆所有对象都在这里分配内存，是垃圾收集的主要区域（”GC 堆”）。 现代的垃圾收集器基本都是采用分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法。可以将堆分成两块： 新生代（Young Generation） 老年代（Old Generation） 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 可以通过 -Xms 和 -Xmx 这两个虚拟机参数来指定一个程序的堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 1java -Xms1M -Xmx2M HackTheJava 5、方法区 用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法去描述为堆的一个逻辑部分，但是它却有一个别名叫做Non0Heap（非堆），目的应该是与Java堆区分开。 和堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。HotSpot 虚拟机把它当成永久代来进行垃圾回收。但很难确定永久代的大小，因为它受到很多因素影响，并且每次 Full GC 之后永久代的大小都会改变，所以经常会抛出 OutOfMemoryError 异常。为了更容易管理方法区，从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 6、运行时常量池 运行时常量池是方法区的一部分。 Class 文件中的常量池（编译器生成的字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern( )。 7、直接内存 在 JDK 1.4 中新引入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在堆内存和堆外内存来回拷贝数据。 参考文章：周志明《深入理解Java虚拟机：JVM高级特性与最佳实战》]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>Java面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的锁分类]]></title>
    <url>%2FJava%E4%B8%AD%E7%9A%84%E9%94%81%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Java中常见的锁分类​ 在Java高并发中，锁是一个很重要的概念，以下有很多锁的名词，当然这些分类并不是全是指锁的状态，有的是指锁的特性，有的是指锁的设计，下面总结每种锁都有一些什么特点。 公平锁/非公平锁 可重入锁 独享锁/共享锁 互斥锁/读写锁 乐观锁/悲观锁 分段锁 偏向锁/轻量级锁/重量级锁 自旋锁 ##公平锁/非公平锁 ​ 公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。对于Java中的 ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。 可重入锁​ 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。说的有点抽象，下面会有一个代码的示例。对于Java中的 ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是ReentrantLock重新进入锁。对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。 独享锁/共享锁​ 独享锁是指该锁一次只能被一个线程所持有。共享锁是指该锁可被多个线程所持有。对于Java 中的ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。对于Synchronized而言，当然是独享锁。 12345678synchronized void setA() throws Exception&#123; Thread.sleep(1000); setB();&#125;synchronized void setB() throws Exception&#123; Thread.sleep(1000);&#125; ​ 上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。 互斥锁/读写锁上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。 互斥锁在Java中的具体实现就是ReentrantLock 读写锁在Java中的具体实现就是ReadWriteLock 乐观锁/悲观锁​ 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。 分段锁​ 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock)。当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁​ 这三种锁是指锁的状态，并且是针对Synchronized。在Java 5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。 ​ 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁以降低获取锁的代价。 轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能.重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁​ 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java高并发</tag>
        <tag>Java锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇测试博客]]></title>
    <url>%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[一级标题测试java代码： 12345public void main()&#123; public static void main(String[] args)&#123; System.out.println("Hello,MyBlog!"); &#125;&#125; 二级标题你好！ 参考文献www.bestzuo.cn]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
</search>
